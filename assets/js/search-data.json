{
  
    
        "post0": {
            "title": "Predicting rain in Australia",
            "content": "INTRODUCTION . In this project, we will compare 2 machine learning models, namely Logistic Regression and XGBoost , and also explain the use for each algorithm. The dataset used for this project is called weatherAUS . First, let&#39;s import some basic libraries used for processing data and visualisation. . import numpy as np #For linear algebra import pandas as pd #For working with dataset import matplotlib import matplotlib.pyplot as plt #Visualisation import seaborn as sns #Visualisation . I. Data exploration . Read the dataset and inspect its appearance. . df = pd.read_csv(&#39;./input/weatherAUS.csv&#39;) df.head() . Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am ... Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow . 0 2008-12-01 | Albury | 13.4 | 22.9 | 0.6 | NaN | NaN | W | 44.0 | W | ... | 71.0 | 22.0 | 1007.7 | 1007.1 | 8.0 | NaN | 16.9 | 21.8 | No | No | . 1 2008-12-02 | Albury | 7.4 | 25.1 | 0.0 | NaN | NaN | WNW | 44.0 | NNW | ... | 44.0 | 25.0 | 1010.6 | 1007.8 | NaN | NaN | 17.2 | 24.3 | No | No | . 2 2008-12-03 | Albury | 12.9 | 25.7 | 0.0 | NaN | NaN | WSW | 46.0 | W | ... | 38.0 | 30.0 | 1007.6 | 1008.7 | NaN | 2.0 | 21.0 | 23.2 | No | No | . 3 2008-12-04 | Albury | 9.2 | 28.0 | 0.0 | NaN | NaN | NE | 24.0 | SE | ... | 45.0 | 16.0 | 1017.6 | 1012.8 | NaN | NaN | 18.1 | 26.5 | No | No | . 4 2008-12-05 | Albury | 17.5 | 32.3 | 1.0 | NaN | NaN | W | 41.0 | ENE | ... | 82.0 | 33.0 | 1010.8 | 1006.0 | 7.0 | 8.0 | 17.8 | 29.7 | No | No | . 5 rows × 23 columns . print(df.columns) print(len(df.columns)) . Index([&#39;Date&#39;, &#39;Location&#39;, &#39;MinTemp&#39;, &#39;MaxTemp&#39;, &#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;Sunshine&#39;, &#39;WindGustDir&#39;, &#39;WindGustSpeed&#39;, &#39;WindDir9am&#39;, &#39;WindDir3pm&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;, &#39;Humidity9am&#39;, &#39;Humidity3pm&#39;, &#39;Pressure9am&#39;, &#39;Pressure3pm&#39;, &#39;Cloud9am&#39;, &#39;Cloud3pm&#39;, &#39;Temp9am&#39;, &#39;Temp3pm&#39;, &#39;RainToday&#39;, &#39;RainTomorrow&#39;], dtype=&#39;object&#39;) 23 . In this dataset, we have 23 columns, including the target RainTomorrow variable. . Let&#39;s have a look at the dataset information . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 145460 entries, 0 to 145459 Data columns (total 23 columns): # Column Non-Null Count Dtype -- -- 0 Date 145460 non-null object 1 Location 145460 non-null object 2 MinTemp 143975 non-null float64 3 MaxTemp 144199 non-null float64 4 Rainfall 142199 non-null float64 5 Evaporation 82670 non-null float64 6 Sunshine 75625 non-null float64 7 WindGustDir 135134 non-null object 8 WindGustSpeed 135197 non-null float64 9 WindDir9am 134894 non-null object 10 WindDir3pm 141232 non-null object 11 WindSpeed9am 143693 non-null float64 12 WindSpeed3pm 142398 non-null float64 13 Humidity9am 142806 non-null float64 14 Humidity3pm 140953 non-null float64 15 Pressure9am 130395 non-null float64 16 Pressure3pm 130432 non-null float64 17 Cloud9am 89572 non-null float64 18 Cloud3pm 86102 non-null float64 19 Temp9am 143693 non-null float64 20 Temp3pm 141851 non-null float64 21 RainToday 142199 non-null object 22 RainTomorrow 142193 non-null object dtypes: float64(16), object(7) memory usage: 25.5+ MB . We can see that our dataset includes both numerical and categorical variables. And there are also missing values in our dataset since the number of non-null values doesn&#39;t match the number of entries. . We create 2 variable called categorical and numerical to make it easier for inspecting the columns given their different characteristics. . # dtpyes = &#39;0&#39; means object categorical = [i for i in df.columns if df[i].dtypes == &#39;O&#39;] # List of numerical variables numerical = [i for i in df.columns if i not in categorical] print(&#39;categorical:&#39;, categorical, &#39; n&#39;, &#39;numerical: &#39;, numerical) . categorical: [&#39;Date&#39;, &#39;Location&#39;, &#39;WindGustDir&#39;, &#39;WindDir9am&#39;, &#39;WindDir3pm&#39;, &#39;RainToday&#39;, &#39;RainTomorrow&#39;] numerical: [&#39;MinTemp&#39;, &#39;MaxTemp&#39;, &#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;Sunshine&#39;, &#39;WindGustSpeed&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;, &#39;Humidity9am&#39;, &#39;Humidity3pm&#39;, &#39;Pressure9am&#39;, &#39;Pressure3pm&#39;, &#39;Cloud9am&#39;, &#39;Cloud3pm&#39;, &#39;Temp9am&#39;, &#39;Temp3pm&#39;] . Let&#39;s check how many null values are there in each variable . df[numerical].isnull().sum() . MinTemp 1485 MaxTemp 1261 Rainfall 3261 Evaporation 62790 Sunshine 69835 WindGustSpeed 10263 WindSpeed9am 1767 WindSpeed3pm 3062 Humidity9am 2654 Humidity3pm 4507 Pressure9am 15065 Pressure3pm 15028 Cloud9am 55888 Cloud3pm 59358 Temp9am 1767 Temp3pm 3609 dtype: int64 . df[numerical].isnull().sum().plot.bar() plt.show() . As can be seen, Evaporation, Sunshine, Cloud9am and Cloud3pm are the ones with the highest number of missing values in numerical segment. . entries = 145460 round(df[numerical].isnull().sum() / entries, 3) . MinTemp 0.010 MaxTemp 0.009 Rainfall 0.022 Evaporation 0.432 Sunshine 0.480 WindGustSpeed 0.071 WindSpeed9am 0.012 WindSpeed3pm 0.021 Humidity9am 0.018 Humidity3pm 0.031 Pressure9am 0.104 Pressure3pm 0.103 Cloud9am 0.384 Cloud3pm 0.408 Temp9am 0.012 Temp3pm 0.025 dtype: float64 . Approximately 48% values of Sunshine is missing while the rate for Evaporation,Cloud9am and Cloud3pm is at around 40%. . (df[numerical].isnull().sum() / entries).plot.bar() plt.show() . Similarly, we calculate and plot the number of missing values in categorical variables . df[categorical].isnull().sum() . Date 0 Location 0 WindGustDir 10326 WindDir9am 10566 WindDir3pm 4228 RainToday 3261 RainTomorrow 3267 dtype: int64 . df[categorical].isnull().sum().plot.bar() plt.show() . round(df[categorical].isnull().sum() / entries, 3) . Date 0.000 Location 0.000 WindGustDir 0.071 WindDir9am 0.073 WindDir3pm 0.029 RainToday 0.022 RainTomorrow 0.022 dtype: float64 . (df[categorical].isnull().sum() / entries).plot.bar() plt.show() . The missing rate in categorical variable is not much since the highest one is around 7% of WindDir9am. On the other hand, Date, Location and especially our target variable RainTomorrow show no sign of missing values . Let&#39;s inspect the distribution of numerical variables. . df[numerical].hist(bins=10, figsize = (20,15)) plt.show() . /Users/xucheng/dev/da-venv/lib/python3.9/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): . The histograms show us that some variables follow normal distribution, for example Humidity3pm, MinTemp, Temp9am while some are highly skewed, namely Evaporation, Rainfall. We will process these values later before using them for the model. . Some descriptive statistics about categorical variables . df[numerical].dtypes . MinTemp float64 MaxTemp float64 Rainfall float64 Evaporation float64 Sunshine float64 WindGustSpeed float64 WindSpeed9am float64 WindSpeed3pm float64 Humidity9am float64 Humidity3pm float64 Pressure9am float64 Pressure3pm float64 Cloud9am float64 Cloud3pm float64 Temp9am float64 Temp3pm float64 dtype: object . df[categorical].describe() . Date Location WindGustDir WindDir9am WindDir3pm RainToday RainTomorrow . count 145460 | 145460 | 135134 | 134894 | 141232 | 142199 | 142193 | . unique 3436 | 49 | 16 | 16 | 16 | 2 | 2 | . top 2014-04-12 | Canberra | W | N | SE | No | No | . freq 49 | 3436 | 9915 | 11758 | 10838 | 110319 | 110316 | . From the summary table, we can see the most frequent values of each variable and its frequency in the dataset. . Here, we have 2 binary-class variables RainToday and RainTomorrow. Let&#39;s see the values of these two variables. . print(df[&#39;RainToday&#39;].value_counts()) print(df[&#39;RainTomorrow&#39;].value_counts()) . No 110319 Yes 31880 Name: RainToday, dtype: int64 No 110316 Yes 31877 Name: RainTomorrow, dtype: int64 . %matplotlib inline plt.figure(figsize=(10,5)) plt.subplot(1,2,1) df[&#39;RainToday&#39;].value_counts().plot.bar() plt.title(&#39;RainToday&#39;) plt.subplot(1,2,2) df[&#39;RainTomorrow&#39;].value_counts().plot.bar(color=&#39;g&#39;) plt.title(&#39;RainTomorrow&#39;) plt.show() . /Users/xucheng/dev/da-venv/lib/python3.9/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): . As we can see, both of the graph show the same proportion between its values. In our target variable RainTomorrow, there is a big difference between the number of yes-no values. We have to take this into consideration when sampling to avoid biasness in the model. . Let&#39;s have a look at some statistics of the dataset . df[numerical].describe() . MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm . count 143975.000000 | 144199.000000 | 142199.000000 | 82670.000000 | 75625.000000 | 135197.000000 | 143693.000000 | 142398.000000 | 142806.000000 | 140953.000000 | 130395.00000 | 130432.000000 | 89572.000000 | 86102.000000 | 143693.000000 | 141851.00000 | . mean 12.194034 | 23.221348 | 2.360918 | 5.468232 | 7.611178 | 40.035230 | 14.043426 | 18.662657 | 68.880831 | 51.539116 | 1017.64994 | 1015.255889 | 4.447461 | 4.509930 | 16.990631 | 21.68339 | . std 6.398495 | 7.119049 | 8.478060 | 4.193704 | 3.785483 | 13.607062 | 8.915375 | 8.809800 | 19.029164 | 20.795902 | 7.10653 | 7.037414 | 2.887159 | 2.720357 | 6.488753 | 6.93665 | . min -8.500000 | -4.800000 | 0.000000 | 0.000000 | 0.000000 | 6.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 980.50000 | 977.100000 | 0.000000 | 0.000000 | -7.200000 | -5.40000 | . 25% 7.600000 | 17.900000 | 0.000000 | 2.600000 | 4.800000 | 31.000000 | 7.000000 | 13.000000 | 57.000000 | 37.000000 | 1012.90000 | 1010.400000 | 1.000000 | 2.000000 | 12.300000 | 16.60000 | . 50% 12.000000 | 22.600000 | 0.000000 | 4.800000 | 8.400000 | 39.000000 | 13.000000 | 19.000000 | 70.000000 | 52.000000 | 1017.60000 | 1015.200000 | 5.000000 | 5.000000 | 16.700000 | 21.10000 | . 75% 16.900000 | 28.200000 | 0.800000 | 7.400000 | 10.600000 | 48.000000 | 19.000000 | 24.000000 | 83.000000 | 66.000000 | 1022.40000 | 1020.000000 | 7.000000 | 7.000000 | 21.600000 | 26.40000 | . max 33.900000 | 48.100000 | 371.000000 | 145.000000 | 14.500000 | 135.000000 | 130.000000 | 87.000000 | 100.000000 | 100.000000 | 1041.00000 | 1039.600000 | 9.000000 | 9.000000 | 40.200000 | 46.70000 | . The table shows that variables are of different ranges . Let&#39;s draw boxplots to inspect the range of the data and anomalies. . df[numerical].boxplot(figsize=(30,20)) plt.show() . The boxplots reveal a great number of outliers in Rainfall, Evaporation, WindGustSpeed, WindSpeed9am and WindSpeed3pm . outlier = [&#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;WindGustSpeed&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;] df[outlier].hist(bins=10, figsize=(15,10)) plt.show() . /Users/xucheng/dev/da-venv/lib/python3.9/site-packages/pandas/plotting/_matplotlib/tools.py:400: MatplotlibDeprecationWarning: The is_first_col function was deprecated in Matplotlib 3.4 and will be removed two minor releases later. Use ax.get_subplotspec().is_first_col() instead. if ax.is_first_col(): . We can see that the distribution of those suspected variable is skewed and need to be taken care of . Let&#39;s draw several graphs to see the correlation between variables . _ = sns.relplot(x=&quot;MinTemp&quot;,y=&quot;MaxTemp&quot;,hue=&quot;Rainfall&quot;,palette=&quot;viridis&quot;,data=df,s=100) . _ = sns.relplot(x=&quot;WindSpeed3pm&quot;,y=&quot;WindSpeed9am&quot;,hue=&quot;Rainfall&quot;,palette=&quot;viridis&quot;,data=df,s=300) . fig=plt.figure(figsize=(20,8),facecolor=&#39;white&#39;) gs=fig.add_gridspec(1,2) ax=[_ for _ in range(2)] ax[0]=fig.add_subplot(gs[0,0]) ax[1]=fig.add_subplot(gs[0,1]) #ax[0].text(-1,15,&quot;Relationship between Sunshine and Rainfall&quot;,fontsize=21,fontweight=&#39;bold&#39;, fontfamily=&#39;monospace&#39;) #ax[0].text(-1,14.3,&quot;When sunshine increases, rainfall decreases&quot;,fontsize=17,fontweight=&#39;light&#39;, fontfamily=&#39;monospace&#39;) #ax[1].text(-1,430,&quot;Relationship between Wind and Rainfall&quot;,fontsize=21,fontweight=&#39;bold&#39;, fontfamily=&#39;monospace&#39;) #ax[1].text(-1,410,&quot;There is not a clear relationship between wind and rainfall&quot;,fontsize=17,fontweight=&#39;light&#39;, fontfamily=&#39;monospace&#39;) sns.lineplot(data=df,x=&#39;Sunshine&#39;,y=&#39;Rainfall&#39;,ax=ax[0],color=&#39;goldenrod&#39;) sns.scatterplot(data=df,x=&#39;WindGustSpeed&#39;,y=&#39;Rainfall&#39;,ax=ax[1],color=&#39;salmon&#39;) for i in range(2): ax[i].set_ylabel(&#39;Rainfall&#39;).set_rotation(0) ax[i].set_yticklabels(&#39;&#39;) ax[i].tick_params(axis=&#39;y&#39;,length=0) for direction in [&#39;top&#39;,&#39;right&#39;,&#39;left&#39;]: ax[i].spines[direction].set_visible(False) plt.tight_layout() plt.show() . fig, ax = plt.subplots(figsize=(10,10)) cm = sns.heatmap(df.corr(), linewidths = .5, cmap=&quot;YlGnBu&quot;, annot=True, ax=ax, fmt=&#39;.1g&#39;) . From this heatmap, we can see blocks of highly correlated variables along the main diagonal. . Since we want to see if there is any correlation between the date, month or season and the target variable, we will create new variable in the dataset by extracting corresponding data from Date variable. . df[&#39;Date&#39;] = pd.to_datetime(df[&#39;Date&#39;]) df.head() . Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am ... Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow . 0 2008-12-01 | Albury | 13.4 | 22.9 | 0.6 | NaN | NaN | W | 44.0 | W | ... | 71.0 | 22.0 | 1007.7 | 1007.1 | 8.0 | NaN | 16.9 | 21.8 | No | No | . 1 2008-12-02 | Albury | 7.4 | 25.1 | 0.0 | NaN | NaN | WNW | 44.0 | NNW | ... | 44.0 | 25.0 | 1010.6 | 1007.8 | NaN | NaN | 17.2 | 24.3 | No | No | . 2 2008-12-03 | Albury | 12.9 | 25.7 | 0.0 | NaN | NaN | WSW | 46.0 | W | ... | 38.0 | 30.0 | 1007.6 | 1008.7 | NaN | 2.0 | 21.0 | 23.2 | No | No | . 3 2008-12-04 | Albury | 9.2 | 28.0 | 0.0 | NaN | NaN | NE | 24.0 | SE | ... | 45.0 | 16.0 | 1017.6 | 1012.8 | NaN | NaN | 18.1 | 26.5 | No | No | . 4 2008-12-05 | Albury | 17.5 | 32.3 | 1.0 | NaN | NaN | W | 41.0 | ENE | ... | 82.0 | 33.0 | 1010.8 | 1006.0 | 7.0 | 8.0 | 17.8 | 29.7 | No | No | . 5 rows × 23 columns . df[&#39;Day&#39;] = df[&#39;Date&#39;].dt.day df[&#39;Month&#39;] = df[&#39;Date&#39;].dt.month df.head() . Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am ... Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow Day Month . 0 2008-12-01 | Albury | 13.4 | 22.9 | 0.6 | NaN | NaN | W | 44.0 | W | ... | 1007.7 | 1007.1 | 8.0 | NaN | 16.9 | 21.8 | No | No | 1 | 12 | . 1 2008-12-02 | Albury | 7.4 | 25.1 | 0.0 | NaN | NaN | WNW | 44.0 | NNW | ... | 1010.6 | 1007.8 | NaN | NaN | 17.2 | 24.3 | No | No | 2 | 12 | . 2 2008-12-03 | Albury | 12.9 | 25.7 | 0.0 | NaN | NaN | WSW | 46.0 | W | ... | 1007.6 | 1008.7 | NaN | 2.0 | 21.0 | 23.2 | No | No | 3 | 12 | . 3 2008-12-04 | Albury | 9.2 | 28.0 | 0.0 | NaN | NaN | NE | 24.0 | SE | ... | 1017.6 | 1012.8 | NaN | NaN | 18.1 | 26.5 | No | No | 4 | 12 | . 4 2008-12-05 | Albury | 17.5 | 32.3 | 1.0 | NaN | NaN | W | 41.0 | ENE | ... | 1010.8 | 1006.0 | 7.0 | 8.0 | 17.8 | 29.7 | No | No | 5 | 12 | . 5 rows × 25 columns . df[&#39;Season&#39;] = df[&#39;Month&#39;].replace((1,2,3,4,5,6,7,8,9,10,11,12), (&#39;Summer&#39;, &#39;Summer&#39;, &#39;Autumn&#39;, &#39;Autumn&#39;, &#39;Autumn&#39;, &#39;Winter&#39;, &#39;Winter&#39;, &#39;Winter&#39;, &#39;Spring&#39;, &#39;Spring&#39;, &#39;Spring&#39;, &#39;Summer&#39;)) df.head() . Date Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am ... Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow Day Month Season . 0 2008-12-01 | Albury | 13.4 | 22.9 | 0.6 | NaN | NaN | W | 44.0 | W | ... | 1007.1 | 8.0 | NaN | 16.9 | 21.8 | No | No | 1 | 12 | Summer | . 1 2008-12-02 | Albury | 7.4 | 25.1 | 0.0 | NaN | NaN | WNW | 44.0 | NNW | ... | 1007.8 | NaN | NaN | 17.2 | 24.3 | No | No | 2 | 12 | Summer | . 2 2008-12-03 | Albury | 12.9 | 25.7 | 0.0 | NaN | NaN | WSW | 46.0 | W | ... | 1008.7 | NaN | 2.0 | 21.0 | 23.2 | No | No | 3 | 12 | Summer | . 3 2008-12-04 | Albury | 9.2 | 28.0 | 0.0 | NaN | NaN | NE | 24.0 | SE | ... | 1012.8 | NaN | NaN | 18.1 | 26.5 | No | No | 4 | 12 | Summer | . 4 2008-12-05 | Albury | 17.5 | 32.3 | 1.0 | NaN | NaN | W | 41.0 | ENE | ... | 1006.0 | 7.0 | 8.0 | 17.8 | 29.7 | No | No | 5 | 12 | Summer | . 5 rows × 26 columns . Now the data already have the features that we want and the Date variable is of no use, we will get rid of this variable by dropping it. . df.drop(&#39;Date&#39;, axis = 1, inplace = True) categorical = [i for i in df.columns if i not in numerical] #Update the list of categorical variables categorical . [&#39;Location&#39;, &#39;WindGustDir&#39;, &#39;WindDir9am&#39;, &#39;WindDir3pm&#39;, &#39;RainToday&#39;, &#39;RainTomorrow&#39;, &#39;Day&#39;, &#39;Month&#39;, &#39;Season&#39;] . We plot the correlation matrix again to see the new values. . fig, ax = plt.subplots(figsize=(10,10)) cm = sns.heatmap(df.corr(), linewidths = .5, cmap=&quot;YlGnBu&quot;, annot=True, ax=ax, fmt=&#39;.1g&#39;) . As can be seen from the heatmap, there are strong correlations between variables of the same type, for example Pressure9am and Pressure3pm, Temp9am and Temp3pm and so on. . II. Data preparation . In this part, we will prepare the data to better expose the underlying data patterns. . 1. Data cleaning . In the data cleaning task, we will come to remove outliers and deal with missing values. . In this case, we will remove the outliers that initially detected in the boxplots. To determine whether a datapoint is outlier or not, we will calculate the interquartile method. . An interquartile will be calculated by the formula: IQR = Q3 - Q1 . IQR_o = df[outlier].quantile(0.75) - df[outlier].quantile(0.25) print(IQR_o) . Rainfall 0.8 Evaporation 4.8 WindGustSpeed 17.0 WindSpeed9am 12.0 WindSpeed3pm 11.0 dtype: float64 . We then iterately compute the upper bound and lower bound of each variable. . upb_rf = round(df[&#39;Rainfall&#39;].quantile(0.25) - 1.5*IQR_o[0], 3) lob_rf = round(df[&#39;Rainfall&#39;].quantile(0.75) + 1.5*IQR_o[0], 3) print(&#39;Upper bound is:&#39;, upb_rf) print(&#39;Lower bound is:&#39;, lob_rf) . Upper bound is: -1.2 Lower bound is: 2.0 . upb_ev = round(df[&#39;Evaporation&#39;].quantile(0.25) - 1.5*IQR_o[1], 3) lob_ev = round(df[&#39;Evaporation&#39;].quantile(0.75) + 1.5*IQR_o[1], 3) print(&#39;Upper bound is:&#39;, upb_ev) print(&#39;Lower bound is:&#39;, lob_ev) . Upper bound is: -4.6 Lower bound is: 14.6 . upb_wg = round(df[&#39;WindGustSpeed&#39;].quantile(0.25) - 1.5*IQR_o[2], 3) lob_wg = round(df[&#39;WindGustSpeed&#39;].quantile(0.75) + 1.5*IQR_o[2], 3) print(&#39;Upper bound is:&#39;, upb_wg) print(&#39;Lower bound is:&#39;, lob_wg) . Upper bound is: 5.5 Lower bound is: 73.5 . upb_ws9 = round(df[&#39;WindSpeed9am&#39;].quantile(0.25) - 1.5*IQR_o[3], 3) lob_ws9 = round(df[&#39;WindSpeed9am&#39;].quantile(0.75) + 1.5*IQR_o[3], 3) print(&#39;Upper bound is:&#39;, upb_ws9) print(&#39;Lower bound is:&#39;, lob_ws9) . Upper bound is: -11.0 Lower bound is: 37.0 . upb_ws3 = round(df[&#39;WindSpeed3pm&#39;].quantile(0.25) - 1.5*IQR_o[4], 3) lob_ws3 = round(df[&#39;WindSpeed3pm&#39;].quantile(0.75) + 1.5*IQR_o[4], 3) print(&#39;Upper bound is:&#39;, upb_ws3) print(&#39;Lower bound is:&#39;, lob_ws3) . Upper bound is: -3.5 Lower bound is: 40.5 . Let&#39;s look at the statistics of these variable again. . df[outlier].describe() . Rainfall Evaporation WindGustSpeed WindSpeed9am WindSpeed3pm . count 142199.000000 | 82670.000000 | 135197.000000 | 143693.000000 | 142398.000000 | . mean 2.360918 | 5.468232 | 40.035230 | 14.043426 | 18.662657 | . std 8.478060 | 4.193704 | 13.607062 | 8.915375 | 8.809800 | . min 0.000000 | 0.000000 | 6.000000 | 0.000000 | 0.000000 | . 25% 0.000000 | 2.600000 | 31.000000 | 7.000000 | 13.000000 | . 50% 0.000000 | 4.800000 | 39.000000 | 13.000000 | 19.000000 | . 75% 0.800000 | 7.400000 | 48.000000 | 19.000000 | 24.000000 | . max 371.000000 | 145.000000 | 135.000000 | 130.000000 | 87.000000 | . As we can see:1. The minimum of Rainfall is 0 and maximum is 371, therefore, the outliers will be values that &gt; 22. Similarly, the outliers values in Evaporation will be &gt; 14.6 . The outliers in WindGustSpeed will be values that are &gt; 73.5 | For WindSpeed9am, the outlier values will be &gt; 37 | And the outliers in WindSpeed3pm will be &gt; 40.5 | Let&#39;s come back to our dataset and get rid of these values . df = df[~((df [&#39;Rainfall&#39;] &gt; 2) |(df[&#39;Evaporation&#39;] &gt; 14.6) | (df[&#39;WindGustSpeed&#39;] &gt; 73.5) | (df[&#39;WindSpeed9am&#39;] &gt; 37) | (df[&#39;WindSpeed3pm&#39;] &gt; 40.5))] df.describe() . MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustSpeed WindSpeed9am WindSpeed3pm Humidity9am Humidity3pm Pressure9am Pressure3pm Cloud9am Cloud3pm Temp9am Temp3pm Day Month . count 113040.000000 | 113258.000000 | 111285.000000 | 64400.00000 | 58932.000000 | 106379.000000 | 112831.000000 | 111899.000000 | 112208.000000 | 110792.000000 | 102153.000000 | 102186.000000 | 68736.000000 | 66022.000000 | 112875.000000 | 111479.000000 | 114362.000000 | 114362.000000 | . mean 11.852334 | 23.719173 | 0.155420 | 5.32550 | 8.125137 | 37.791481 | 13.034671 | 17.748041 | 66.701082 | 48.559598 | 1018.597092 | 1015.938464 | 4.067316 | 4.176956 | 17.063289 | 22.208154 | 15.679623 | 6.376436 | . std 6.438439 | 6.959421 | 0.392512 | 3.13507 | 3.623704 | 11.377243 | 7.983696 | 7.884287 | 18.485727 | 19.625390 | 6.689984 | 6.711610 | 2.887532 | 2.734683 | 6.489946 | 6.764848 | 8.785983 | 3.423026 | . min -8.500000 | -4.100000 | 0.000000 | 0.00000 | 0.000000 | 6.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 984.600000 | 980.200000 | 0.000000 | 0.000000 | -5.600000 | -4.400000 | 1.000000 | 1.000000 | . 25% 7.200000 | 18.500000 | 0.000000 | 2.80000 | 5.800000 | 30.000000 | 7.000000 | 11.000000 | 55.000000 | 34.000000 | 1014.000000 | 1011.300000 | 1.000000 | 1.000000 | 12.400000 | 17.200000 | 8.000000 | 3.000000 | . 50% 11.700000 | 23.300000 | 0.000000 | 5.00000 | 9.100000 | 37.000000 | 13.000000 | 17.000000 | 67.000000 | 49.000000 | 1018.400000 | 1015.800000 | 4.000000 | 4.000000 | 16.900000 | 21.700000 | 16.000000 | 6.000000 | . 75% 16.500000 | 28.700000 | 0.000000 | 7.40000 | 10.900000 | 44.000000 | 19.000000 | 22.000000 | 80.000000 | 62.000000 | 1023.100000 | 1020.500000 | 7.000000 | 7.000000 | 21.700000 | 26.900000 | 23.000000 | 9.000000 | . max 33.900000 | 47.300000 | 2.000000 | 14.60000 | 14.500000 | 72.000000 | 37.000000 | 39.000000 | 100.000000 | 100.000000 | 1041.000000 | 1039.600000 | 9.000000 | 9.000000 | 40.200000 | 46.700000 | 31.000000 | 12.000000 | . df[outlier].describe() . Rainfall Evaporation WindGustSpeed WindSpeed9am WindSpeed3pm . count 111285.000000 | 64400.00000 | 106379.000000 | 112831.000000 | 111899.000000 | . mean 0.155420 | 5.32550 | 37.791481 | 13.034671 | 17.748041 | . std 0.392512 | 3.13507 | 11.377243 | 7.983696 | 7.884287 | . min 0.000000 | 0.00000 | 6.000000 | 0.000000 | 0.000000 | . 25% 0.000000 | 2.80000 | 30.000000 | 7.000000 | 11.000000 | . 50% 0.000000 | 5.00000 | 37.000000 | 13.000000 | 17.000000 | . 75% 0.000000 | 7.40000 | 44.000000 | 19.000000 | 22.000000 | . max 2.000000 | 14.60000 | 72.000000 | 37.000000 | 39.000000 | . Now that the statitics have changed, let&#39;s see if we finally get rid of the outliers. . df[outlier].boxplot(figsize = (10,5)) plt.show() . We can still spot some outliers here, but not many. Why? Because after removing outliers, the statistics values of the dataset will change. In this case we are using interquartile (IQR), which means we are using median values. The median values will change a little and not much affected by the removed outliers. That&#39;s why the new range, in other words, upper bound and lower bound will slightly change. . Now that we get rid of outliers, let&#39;s handle missing values. Let&#39;s have a look again at the number of missing values! . As you can see, we have missing values in both numerical and categorical values so first we will handle categorical attributes and then numerical. After that, we will create a pipeline for feature engineering. . To fill missing values in numerical variables, we will make use of the SimpleImputer function in Scikit learn library. We will replace missing values with the median value of that feature. . from sklearn.impute import SimpleImputer imputer_num = SimpleImputer(strategy=&quot;median&quot;) imputer_num.fit(df[numerical]) . SimpleImputer(strategy=&#39;median&#39;) . df[numerical] = imputer_num.transform(df[numerical]) . Similarly, we use the most frequent value to fill in the place of missing values in categorical variables. . imputer_cat = SimpleImputer(strategy=&quot;most_frequent&quot;) imputer_cat.fit(df[categorical]) df[categorical] = imputer_cat.transform(df[categorical]) . df.isnull().sum() . Location 0 MinTemp 0 MaxTemp 0 Rainfall 0 Evaporation 0 Sunshine 0 WindGustDir 0 WindGustSpeed 0 WindDir9am 0 WindDir3pm 0 WindSpeed9am 0 WindSpeed3pm 0 Humidity9am 0 Humidity3pm 0 Pressure9am 0 Pressure3pm 0 Cloud9am 0 Cloud3pm 0 Temp9am 0 Temp3pm 0 RainToday 0 RainTomorrow 0 Day 0 Month 0 Season 0 dtype: int64 . Now that there is no missing values in our dataset, let&#39;s move on to feature selection task . 2. Feature selection . In this part, we will deal with highly correlated variables . Let&#39;s have a look at the correlation matrix again: . fig, ax = plt.subplots(figsize=(10,10)) cm = sns.heatmap(df.corr(), linewidths = .5, cmap=&quot;YlGnBu&quot;, annot=True, ax=ax, fmt=&#39;.1g&#39;) . As you can see, there&#39;s a perfect correlation between Pressure9am and Pressure3pm, which mean the two variables represent the same information. Therefore, I will get rid of either of them, in this case, Pressure3pm . df.drop(labels=&#39;Pressure3pm&#39;, axis = 1, inplace= True) df.columns . Index([&#39;Location&#39;, &#39;MinTemp&#39;, &#39;MaxTemp&#39;, &#39;Rainfall&#39;, &#39;Evaporation&#39;, &#39;Sunshine&#39;, &#39;WindGustDir&#39;, &#39;WindGustSpeed&#39;, &#39;WindDir9am&#39;, &#39;WindDir3pm&#39;, &#39;WindSpeed9am&#39;, &#39;WindSpeed3pm&#39;, &#39;Humidity9am&#39;, &#39;Humidity3pm&#39;, &#39;Pressure9am&#39;, &#39;Cloud9am&#39;, &#39;Cloud3pm&#39;, &#39;Temp9am&#39;, &#39;Temp3pm&#39;, &#39;RainToday&#39;, &#39;RainTomorrow&#39;, &#39;Day&#39;, &#39;Month&#39;, &#39;Season&#39;], dtype=&#39;object&#39;) . The feature Pressure3pm has already gone! . 3. Feature engineering . Since Machine learning algorithms require input and target variables to be numerical so in this part, we will transform our categorical values into numeric ones. . We will make use of the LabelEncode function from scikit-learn. This function basically assigns values between 0 and number of classes - 1. . For example, [A, B, C] will be decoded as [0, 1, 2]. . from sklearn.preprocessing import LabelEncoder lb_encoder = LabelEncoder() . After calling the function, we will fit and transform the target labels. . for i in df[categorical].columns: df[i] = lb_encoder.fit_transform(df[i]) . Let&#39;s see the result after transformation . for i in df[categorical]: print(i, &#39;:&#39;, df[i].unique()) . Location : [ 2 4 10 11 21 24 26 27 30 34 37 38 42 45 47 9 40 23 5 6 35 19 18 20 25 33 44 12 7 8 14 39 0 22 28 48 1 46 29 32 31 36 43 15 17 3 13 16 41] WindGustDir : [13 14 15 4 3 0 1 10 8 12 6 7 9 5 2 11] WindDir9am : [13 6 9 1 12 10 8 3 11 4 2 0 7 15 14 5] WindDir3pm : [14 15 0 7 13 10 2 6 9 3 12 11 8 5 4 1] RainToday : [0 1] RainTomorrow : [0 1] Day : [ 0 1 2 3 4 5 6 7 9 10 14 15 16 19 20 21 22 23 24 25 26 27 28 29 30 8 11 12 13 17 18] Month : [11 0 1 2 3 4 5 6 7 8 9 10] Season : [2 0 3 1] . df.head() . Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am WindDir3pm ... Pressure9am Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow Day Month Season . 0 2 | 13.4 | 22.9 | 0.6 | 5.0 | 9.1 | 13 | 44.0 | 13 | 14 | ... | 1007.7 | 8.0 | 4.0 | 16.9 | 21.8 | 0 | 0 | 0 | 11 | 2 | . 1 2 | 7.4 | 25.1 | 0.0 | 5.0 | 9.1 | 14 | 44.0 | 6 | 15 | ... | 1010.6 | 4.0 | 4.0 | 17.2 | 24.3 | 0 | 0 | 1 | 11 | 2 | . 2 2 | 12.9 | 25.7 | 0.0 | 5.0 | 9.1 | 15 | 46.0 | 13 | 15 | ... | 1007.6 | 4.0 | 2.0 | 21.0 | 23.2 | 0 | 0 | 2 | 11 | 2 | . 3 2 | 9.2 | 28.0 | 0.0 | 5.0 | 9.1 | 4 | 24.0 | 9 | 0 | ... | 1017.6 | 4.0 | 4.0 | 18.1 | 26.5 | 0 | 0 | 3 | 11 | 2 | . 4 2 | 17.5 | 32.3 | 1.0 | 5.0 | 9.1 | 13 | 41.0 | 1 | 7 | ... | 1010.8 | 7.0 | 8.0 | 17.8 | 29.7 | 0 | 0 | 4 | 11 | 2 | . 5 rows × 24 columns . We can see that our dataset now is purely numerical as required to apply ML algorithms. . 4. Feature scaling . Since the the entries in our dataset are of different scales, we will scale them to a certain range. . Because we only scales the input features, therefore, we create a new variable to store only input features . input_variables = [i for i in df.columns if i != &#39;RainTomorrow&#39;] . In this project, we will use StandardScaler, which redistributes the features with their mean μ = 0 and standard deviation σ =1. . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() df[input_variables] = scaler.fit_transform(df[input_variables]) df.head() . Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am WindDir3pm ... Pressure9am Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow Day Month Season . 0 -1.529824 | 0.242056 | -0.117694 | 1.156575 | -0.07773 | 0.18982 | 1.246665 | 0.570743 | 1.409349 | 1.424197 | ... | -1.720059 | 1.768572 | -0.049123 | -0.024996 | -0.059188 | -0.235018 | 0 | -1.670808 | 1.64287 | 0.471823 | . 1 -1.529824 | -0.695281 | 0.199958 | -0.389780 | -0.07773 | 0.18982 | 1.451439 | 0.570743 | -0.152886 | 1.644484 | ... | -1.261421 | -0.018072 | -0.049123 | 0.021533 | 0.315093 | -0.235018 | 0 | -1.556989 | 1.64287 | 0.471823 | . 2 -1.529824 | 0.163945 | 0.286590 | -0.389780 | -0.07773 | 0.18982 | 1.656214 | 0.752979 | 1.409349 | 1.644484 | ... | -1.735875 | -0.018072 | -1.010821 | 0.610898 | 0.150409 | -0.235018 | 0 | -1.443171 | 1.64287 | 0.471823 | . 3 -1.529824 | -0.414080 | 0.618679 | -0.389780 | -0.07773 | 0.18982 | -0.596307 | -1.251619 | 0.516644 | -1.659821 | ... | -0.154363 | -0.018072 | -0.049123 | 0.161119 | 0.644460 | -0.235018 | 0 | -1.329353 | 1.64287 | 0.471823 | . 4 -1.529824 | 0.882570 | 1.239543 | 2.187479 | -0.07773 | 0.18982 | 1.246665 | 0.297388 | -1.268768 | -0.117812 | ... | -1.229791 | 1.321911 | 1.874274 | 0.114590 | 1.123539 | -0.235018 | 0 | -1.215535 | 1.64287 | 0.471823 | . 5 rows × 24 columns . df.describe() . Location MinTemp MaxTemp Rainfall Evaporation Sunshine WindGustDir WindGustSpeed WindDir9am WindDir3pm ... Pressure9am Cloud9am Cloud3pm Temp9am Temp3pm RainToday RainTomorrow Day Month Season . count 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | ... | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | 114362.00000 | 1.143620e+05 | 1.143620e+05 | 1.143620e+05 | . mean 7.157493e-17 | -1.272443e-16 | 4.453551e-16 | 5.964578e-17 | -1.113388e-16 | 1.749609e-16 | 1.391735e-16 | -1.769491e-16 | 1.590554e-17 | 5.169301e-17 | ... | 2.868564e-14 | -1.908665e-16 | -9.940963e-17 | -7.952770e-18 | 9.861435e-16 | -9.940963e-17 | 0.15687 | -5.582472e-17 | -6.461626e-17 | 8.002475e-17 | . std 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | ... | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | 0.36368 | 1.000004e+00 | 1.000004e+00 | 1.000004e+00 | . min -1.670686e+00 | -3.179226e+00 | -4.016138e+00 | -3.897799e-01 | -2.198062e+00 | -3.248694e+00 | -1.415405e+00 | -2.891745e+00 | -1.491944e+00 | -1.659821e+00 | ... | -5.373350e+00 | -1.804715e+00 | -1.972519e+00 | -3.514660e+00 | -3.981649e+00 | -2.350184e-01 | 0.00000 | -1.670808e+00 | -1.570674e+00 | -1.293174e+00 | . 25% -8.959469e-01 | -7.109038e-01 | -7.385570e-01 | -3.897799e-01 | -3.321699e-01 | 1.520343e-01 | -1.005856e+00 | -7.049107e-01 | -8.224148e-01 | -7.786729e-01 | ... | -6.288168e-01 | -4.647325e-01 | -5.299717e-01 | -7.074194e-01 | -7.179218e-01 | -2.350184e-01 | 0.00000 | -8.740805e-01 | -9.863935e-01 | -1.293174e+00 | . 50% 1.965390e-02 | -2.352291e-02 | -5.993881e-02 | -3.897799e-01 | -7.773006e-02 | 1.898201e-01 | 1.801684e-02 | -6.708408e-02 | -1.528856e-01 | 1.024752e-01 | ... | -2.784260e-02 | -1.807157e-02 | -4.912261e-02 | -2.499627e-02 | -7.415903e-02 | -2.350184e-01 | 0.00000 | 3.646477e-02 | -1.099722e-01 | -4.106756e-01 | . 75% 8.648239e-01 | 7.107248e-01 | 7.053115e-01 | -3.897799e-01 | 9.189651e-02 | 2.276060e-01 | 8.371153e-01 | 5.707425e-01 | 7.398199e-01 | 7.633363e-01 | ... | 6.047618e-01 | 8.752504e-01 | 4.317265e-01 | 7.039557e-01 | 6.744023e-01 | -2.350184e-01 | 0.00000 | 8.331919e-01 | 7.664490e-01 | 4.718228e-01 | . max 1.709994e+00 | 3.444626e+00 | 3.405346e+00 | 4.764737e+00 | 3.993307e+00 | 2.230257e+00 | 1.656214e+00 | 3.122049e+00 | 1.855702e+00 | 1.644484e+00 | ... | 3.546372e+00 | 2.215233e+00 | 2.355123e+00 | 3.588744e+00 | 3.668648e+00 | 4.254987e+00 | 1.00000 | 1.743737e+00 | 1.642870e+00 | 1.354321e+00 | . 8 rows × 24 columns . The range and statistics of the dataset has changed, which mean features have been scaled. . III. Train models . In this part, we will train our model using different ML algorithms. . Since this is a classification task, we will use 2 algorthms in this project, namely: . Logistic Regression | XGBoost Classifier | First, let&#39;s call x our input data and y our target variable, we then split our dataset into train set and test set. . x = df[input_variables] y = df[&#39;RainTomorrow&#39;] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(x, y, stratify = y, test_size=0.2, random_state=5) . Here, we keep 20% of the dataset for testing by assigning test_size=0.2, the random_state is set to 5 to guarantee that each time we run the code, the same sequences of random number are generated. . Remember that when we inspect the target variable RainTomorrow, there is a big gap between the number of observations of 2 classes. Therefore, the dataset will be split using stratified sampling method by setting the parameter stratify = y to ensure the data is representative. . 1. Logistic Regression . Logistic regression is a ML model that is mainly used for classification problems. The model will calculate the probability of the datapoints belong to each class and then make the final decision by taking the highest probability. . Let&#39;s create a Logistic Regression model: . from sklearn.linear_model import LogisticRegression lg_model = LogisticRegression(penalty = &#39;l2&#39;, random_state = 5) lg_model.fit(X_train, y_train) . LogisticRegression(random_state=5) . The logistic regression is created using l2 regularization and random_state=5 . from sklearn.metrics import accuracy_score y_pred_lg = lg_model.predict(X_test) print(accuracy_score(y_test, y_pred_lg)) . 0.8665675687491803 . Here, we will use the accuracy score to see how accurately the model can predict on the test set when trained on the train set. . As displayed, the logistic regression model can predict whether it rains tomorrow or not with the accuracy of 86.6% . Let&#39;s check with the result on train set to see if it is overfitting. . print(&quot;Test score:&quot;, lg_model.score(X_test, y_test)) print(&quot;Train score:&quot;, lg_model.score(X_train, y_train)) . Test score: 0.8665675687491803 Train score: 0.8680059897911224 . The scores on both training set and test set are roughly the same, which mean that this model generalises well with unseen data. . Let&#39;s plot a confusion matrix to see how many cases were predicted correctly/ incorrectly. . from sklearn.metrics import confusion_matrix cfm_lg = confusion_matrix(y_test, y_pred_lg) . sns.heatmap(cfm_lg, annot=True, fmt=&quot;d&quot;, linewidths=.5, cmap = &#39;Reds&#39;) plt.show() . The heatmap shows us that: 18182 of them were **correctly classified as No. 553 were *wrongly classified as Yes***. . 2436 were wrongly classified as No | 1152 were correctly classified as Yes. ! Warning:Each time we run this notebook, the number will be different with the number mentioned above. | . Let&#39;s calculate the precision and recall score for this model. In this case, the &quot;No&quot; will be negative class while &quot;Yes&quot; will be our positive class. . We will use the classification_report from sklearn.metrics library . from sklearn.metrics import classification_report print(classification_report(y_test, y_pred_lg)) . precision recall f1-score support 0 0.88 0.97 0.92 19285 1 0.66 0.30 0.42 3588 accuracy 0.87 22873 macro avg 0.77 0.64 0.67 22873 weighted avg 0.85 0.87 0.84 22873 . We can see that the this model is better at predicting if there is no rain tomorrow since the scores for class 0 are much higher than class 1. . Base on the situation we should ask for a high precision or recall score. . For example, given the bushfire recently happened in Australia, if it rains tomorrow in this area, we can plan to focus the fire fighter teams on the other spot, which means we want to minimize the case that wrongly predict it rains tomorrow in this area. In other words, we want to minimize the FP, which leads to higher precision score. . 2. XGBoost Classifier . In this part, we will initialize an XGBClassifier and train the model. This classifies using eXtreme Gradient Boosting - using gradient boosting algorithms for modern data science problems. It falls under the category of Ensemble Learning in ML, where we train and predict using many models to produce one superior output. XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm, which attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models. . from xgboost import XGBClassifier XGB_model = XGBClassifier() XGB_model.fit(X_train,y_train) . /Users/xucheng/dev/da-venv/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [17:57:54] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;binary:logistic&#39; was changed from &#39;error&#39; to &#39;logloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type=&#39;gain&#39;, interaction_constraints=&#39;&#39;, learning_rate=0.300000012, max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan, monotone_constraints=&#39;()&#39;, n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=&#39;exact&#39;, validate_parameters=1, verbosity=None) . Here, we will take the default value for all the parameters. The number of trees this model will create will be 100 as n_estimators=100. If n_estimator=1, this model will create only 1 tree and basically work as a DecisionTree Classifier algorithm. . Let&#39;s see how well this model can perform! . y_pred = XGB_model.predict(X_test) print(accuracy_score(y_test, y_pred)*100) . 87.52240633060813 . /Users/xucheng/dev/da-venv/lib/python3.9/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption warnings.warn( . Its accuracy is ~87.52%, which is a relatively high score. However, is this the best? Let&#39;s use GridCVSearch to iterate through the list of number of estimators and find out the best one! . from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold n_estimators = range(100, 500, 50) param_grid = dict(n_estimators=n_estimators) kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=5) grid_search = GridSearchCV(XGB_model, param_grid, scoring=&quot;roc_auc&quot;, n_jobs=-1, cv=kfold) grid_result = grid_search.fit(X_train, y_train) . We are doing a grid search on a list of estimators ranging from 100 to 500 with step = 50. In this case, we created 80 models since we have 8 estimators (100, 150, 200, 250, 300, 350, 400, 450) and 10 folds cross validation. The scoring is set to roc_auc to compute the area under the ROC curve (ROC AUC) from prediction scores. n_jobs=-1 means that we will use all the processors to boost the running time. It took about 10 mins to run this grid search on this kernel. . print(&#39;Best score:&#39;, grid_result.best_score_) print(&#39;Best parameter:&#39;, grid_result.best_params_) print(&#39;Best estimator:&#39;, grid_result.best_estimator_) . We can see that the best score is 0.86 when the n_estimators is 450 and other paramaters settings. We will use all these parameters to run our model! . best_model = grid_result.best_estimator_ best_model.score(X_test, y_test) . We can see that the model return an accuracy score of 87.55%, higher than the defaul settings. However, the best score from the grid search is different from the one we run using the best parameters, why this happened? . This can be explained by the fact that we are running our model on the test set whereas in the grid search, we evaluate the model using cross validation. Since we have 10 folds cross validation, the score is the average of the 10 models, therefore, the score cannot be expected to be the same. . Let&#39;s calculate the score on training set to see if the model is overfitting or not! . print(&quot;Test score:&quot;, best_model.score(X_test, y_test)) print(&#39;Train score:&#39;, best_model.score(X_train, y_train)) . The train score is higher than the test score but the gap is not significant so we can conclude that it&#39;s slightly overfitting here. . Since xgboost is a decision tree based ensemble ML algorithm, the model consists of a set of classification and regression trees (CART). Let&#39;s visualise a single decision tree within this trained model to get insight into the gradient boosting process. . from xgboost import plot_tree fig, ax = plt.subplots(figsize=(200, 600)) plot_tree(best_model, num_trees=0, ax=ax) plt.show() . We can see the features and feature values for each split and also the score of the leaf nodes. If the feature values meet the condition or the value is missing, it will come to the right node and left node otherwise. This is the reason why XGBoost can handle missing values in the dataset to minimize the loss. The value in the leaf node is the predicted probability of the datapoint belong to class 1 since our target variable is a binary class. . We can also visualise another decision tree by modifying the parameter num_trees with the tree index that we want to visualise. For example, we will visualise the fifth decision tree with the parameter num_trees = 4 . #plot_tree(best_model, num_trees=4, ax=ax) #plt.show() . Since our best model uses the n_estimator = 450, thus, there will be 450 trees in our XGB model. . Let&#39;s have a look at the AUC ROC of this model . y_pred_xgb = best_model.predict_proba(X_test)[:, 1].reshape(-1,1) . Here we are calculating the probability of the datapoints falling into each class. We just care about the probability it rains tomorrow so we just only extract that data. . Let&#39;s plot the ROC curve . from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_test, y_pred_xgb) plt.figure(figsize=(6,4)) plt.plot(fpr, tpr, linewidth=2) plt.plot([0,1], [0,1], &#39;k--&#39; ) plt.rcParams[&#39;font.size&#39;] = 15 plt.title(&#39;ROC curve&#39;) plt.xlabel(&#39;FP Rate&#39;) plt.ylabel(&#39;TP Rate&#39;) plt.show() . From this ROC curve, we can choose a thresold that help us balance the sensitivity (TP rate) and specificity (TN rate) rate based on our own preference. . Let&#39;s see the area under the ROC curve . from sklearn.metrics import roc_auc_score ROC_AUC = roc_auc_score(y_test, y_pred_xgb) print(&#39;Area under the ROC curve:&#39;, ROC_AUC) . The AUC ROC value shows us that the performance of the model is good to predict if it rains tomorrow. . IV. Conclusion . In this project, we have used 2 different ML algorithms to predict whether it rains tomorrow or not and the performance is as followed: . Logistic Regression: approx. 86.6% | XGBoost Classifier: approx. 87.6% | . Overall, the two models seem to perform well on this dataset with no sign of overfitting. . Predicting rain in Australia . INTRODUCTION . In this project, we will compare 2 machine learning models, namely Logistic Regression and XGBoost , and also explain the use for each algorithm. The dataset used for this project is called weatherAUS . First, let&#39;s import some basic libraries used for processing data and visualisation. . import numpy as np #For linear algebra import pandas as pd #For working with dataset import matplotlib import matplotlib.pyplot as plt #Visualisation import seaborn as sns #Visualisation . I. Data exploration . Read the dataset and inspect its appearance. . df = pd.read_csv(&#39;./input/weatherAUS.csv&#39;) df.head() . print(df.columns) print(len(df.columns)) . In this dataset, we have 23 columns, including the target RainTomorrow variable. . Let&#39;s have a look at the dataset information . df.info() . We can see that our dataset includes both numerical and categorical variables. And there are also missing values in our dataset since the number of non-null values doesn&#39;t match the number of entries. . We create 2 variable called categorical and numerical to make it easier for inspecting the columns given their different characteristics. . # dtpyes = &#39;0&#39; means object categorical = [i for i in df.columns if df[i].dtypes == &#39;O&#39;] # List of numerical variables numerical = [i for i in df.columns if i not in categorical] print(&#39;categorical:&#39;, categorical, &#39; n&#39;, &#39;numerical: &#39;, numerical) . Let&#39;s check how many null values are there in each variable . df[numerical].isnull().sum() . IV. Conclusion . In this project, we have used 2 different ML algorithms to predict whether it rains tomorrow or not and the performance is as followed: . Logistic Regression: approx. 86.6% | XGBoost Classifier: approx. 87.6% | . Overall, the two models seem to perform well on this dataset with no sign of overfitting. .",
            "url": "https://chenghsu.github.io/Data-Analysis-Blog/2021/04/18/rain-in-australia.html",
            "relUrl": "/2021/04/18/rain-in-australia.html",
            "date": " • Apr 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px from fbprophet import Prophet from sklearn.metrics import r2_score plt.style.use(&quot;ggplot&quot;) df0 = pd.read_csv(&quot;./input/CONVENIENT_global_confirmed_cases.csv&quot;) df1 = pd.read_csv(&quot;./input/CONVENIENT_global_deaths.csv&quot;) df2 = pd.read_csv(&quot;./input/CONVENIENT_us_confirmed_cases.csv&quot;) df3 = pd.read_csv(&quot;./input/CONVENIENT_us_deaths.csv&quot;) . /Users/xucheng/dev/da-venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342) have mixed types.Specify dtype option on import or set low_memory=False. . df0 . Country/Region Afghanistan Albania Algeria Andorra Angola Antigua and Barbuda Argentina Armenia Australia ... United Kingdom.11 Uruguay Uzbekistan Vanuatu Venezuela Vietnam West Bank and Gaza Yemen Zambia Zimbabwe . 0 Province/State | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Australian Capital Territory | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 1/23/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 1/24/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 1/25/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 1/26/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 434 3/31/21 | 70.0 | 434.0 | 131.0 | 66.0 | 129.0 | 0.0 | 16056.0 | 1148.0 | 0.0 | ... | 4052.0 | 3088.0 | 187.0 | 0.0 | 1348.0 | 9.0 | 2288.0 | 110.0 | 219.0 | 43.0 | . 435 4/1/21 | 63.0 | 349.0 | 112.0 | 43.0 | 88.0 | 11.0 | 14430.0 | 1097.0 | 0.0 | ... | 4478.0 | 2639.0 | 181.0 | 0.0 | 1254.0 | 14.0 | 2292.0 | 174.0 | 131.0 | 14.0 | . 436 4/2/21 | 55.0 | 336.0 | 125.0 | 62.0 | 68.0 | 5.0 | 9902.0 | 1116.0 | 0.0 | ... | 3402.0 | 3380.0 | 189.0 | 0.0 | 979.0 | 3.0 | 2248.0 | 89.0 | 181.0 | 7.0 | . 437 4/3/21 | 23.0 | 341.0 | 95.0 | 59.0 | 112.0 | 18.0 | 10384.0 | 1192.0 | 0.0 | ... | 3423.0 | 2336.0 | 196.0 | 0.0 | 1607.0 | 6.0 | 1589.0 | 77.0 | 70.0 | 8.0 | . 438 4/4/21 | 81.0 | 348.0 | 98.0 | 57.0 | 52.0 | 0.0 | 9955.0 | 590.0 | 0.0 | ... | 2297.0 | 3853.0 | 188.0 | 0.0 | 1786.0 | 5.0 | 2806.0 | 101.0 | 130.0 | 12.0 | . 439 rows × 275 columns . df1 . Country/Region Afghanistan Albania Algeria Andorra Angola Antigua and Barbuda Argentina Armenia Australia ... United Kingdom.11 Uruguay Uzbekistan Vanuatu Venezuela Vietnam West Bank and Gaza Yemen Zambia Zimbabwe . 0 Province/State | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Australian Capital Territory | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 1/23/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 1/24/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 1/25/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 1/26/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 434 3/31/21 | 8.0 | 8.0 | 4.0 | 0.0 | 1.0 | 0.0 | 122.0 | 18.0 | 0.0 | ... | 43.0 | 21.0 | 1.0 | 0.0 | 13.0 | 0.0 | 13.0 | 6.0 | 6.0 | 3.0 | . 435 4/1/21 | 5.0 | 6.0 | 3.0 | 0.0 | 1.0 | 0.0 | 83.0 | 18.0 | 0.0 | ... | 51.0 | 35.0 | 1.0 | 0.0 | 13.0 | 0.0 | 18.0 | 18.0 | 4.0 | 0.0 | . 436 4/2/21 | 6.0 | 6.0 | 3.0 | 1.0 | 0.0 | 0.0 | 82.0 | 19.0 | 0.0 | ... | 52.0 | 32.0 | 0.0 | 0.0 | 14.0 | 0.0 | 16.0 | 10.0 | 3.0 | 1.0 | . 437 4/3/21 | 1.0 | 9.0 | 3.0 | 1.0 | 2.0 | 0.0 | 83.0 | 23.0 | 0.0 | ... | 10.0 | 30.0 | 0.0 | 0.0 | 18.0 | 0.0 | 20.0 | 16.0 | 0.0 | 0.0 | . 438 4/4/21 | 1.0 | 9.0 | 3.0 | 0.0 | 2.0 | 0.0 | 93.0 | 17.0 | 0.0 | ... | 10.0 | 30.0 | 1.0 | 0.0 | 15.0 | 0.0 | 25.0 | 14.0 | 5.0 | 1.0 | . 439 rows × 275 columns . world = pd.DataFrame({&quot;Country&quot;:[],&quot;Cases&quot;:[]}) world[&quot;Country&quot;] = df_confirmed.iloc[:,1:].columns #world[&quot;Country&quot;] = df_confirmed.iloc[:1,1:].keys() cases = [] world . Country Cases . 0 Afghanistan | NaN | . 1 Albania | NaN | . 2 Algeria | NaN | . 3 Andorra | NaN | . 4 Angola | NaN | . ... ... | ... | . 269 Vietnam | NaN | . 270 West Bank and Gaza | NaN | . 271 Yemen | NaN | . 272 Zambia | NaN | . 273 Zimbabwe | NaN | . 274 rows × 2 columns . print(df0.info()) print(df1.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 439 entries, 0 to 438 Columns: 275 entries, Country/Region to Zimbabwe dtypes: float64(189), object(86) memory usage: 943.3+ KB None &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 439 entries, 0 to 438 Columns: 275 entries, Country/Region to Zimbabwe dtypes: float64(189), object(86) memory usage: 943.3+ KB None . print(df0.describe()) print(df1.describe()) . Afghanistan Albania Algeria Andorra Angola count 438.000000 438.000000 438.000000 438.000000 438.000000 mean 129.397260 288.883562 268.543379 27.924658 51.668950 std 188.258907 329.859387 242.821641 43.353650 64.362052 min 0.000000 0.000000 0.000000 0.000000 0.000000 25% 17.250000 17.000000 112.000000 0.000000 1.000000 50% 59.000000 133.500000 191.500000 8.000000 32.000000 75% 165.750000 536.500000 391.750000 42.750000 77.000000 max 1485.000000 1239.000000 1133.000000 299.000000 355.000000 Antigua and Barbuda Argentina Armenia Austria count 438.000000 438.000000 438.000000 438.000000 mean 2.671233 5464.593607 448.936073 1275.696347 std 8.683430 4757.985846 534.242457 1717.546081 min -1.000000 0.000000 0.000000 0.000000 25% 0.000000 255.750000 83.500000 61.750000 50% 0.000000 5317.000000 254.000000 422.000000 75% 1.000000 8878.500000 579.250000 1919.250000 max 82.000000 18326.000000 2476.000000 9586.000000 Azerbaijan ... United Kingdom.11 Uruguay Uzbekistan count 438.000000 ... 438.000000 438.000000 438.000000 mean 618.116438 ... 9952.940639 268.851598 190.920091 std 1020.377636 ... 13482.519983 547.666469 212.983096 min 0.000000 ... 0.000000 -21.000000 0.000000 25% 76.000000 ... 941.000000 5.000000 33.250000 50% 186.000000 ... 4030.000000 17.000000 98.000000 75% 547.750000 ... 15611.000000 379.750000 298.500000 max 4451.000000 ... 68053.000000 3853.000000 981.000000 Vanuatu Venezuela Vietnam West Bank and Gaza Yemen count 438.000000 438.000000 438.000000 438.000000 438.000000 mean 0.006849 379.276256 6.006849 573.716895 10.954338 std 0.106746 353.331479 11.603379 660.812548 23.463643 min 0.000000 0.000000 0.000000 0.000000 -1.000000 25% 0.000000 16.500000 0.000000 5.250000 0.000000 50% 0.000000 352.000000 2.000000 387.000000 1.000000 75% 0.000000 567.500000 6.000000 761.750000 10.000000 max 2.000000 1786.000000 110.000000 2806.000000 174.000000 Zambia Zimbabwe count 438.000000 438.000000 mean 203.036530 84.299087 std 357.327614 181.512977 min 0.000000 -6.000000 25% 0.000000 1.000000 50% 44.000000 20.000000 75% 213.750000 78.750000 max 1796.000000 1365.000000 [8 rows x 189 columns] Afghanistan Albania Algeria Andorra Angola count 438.000000 438.000000 438.000000 438.000000 438.000000 mean 5.700913 5.171233 7.089041 0.267123 1.237443 std 7.710705 5.638247 5.239685 0.682817 1.659386 min 0.000000 0.000000 0.000000 0.000000 -3.000000 25% 0.000000 0.000000 3.000000 0.000000 0.000000 50% 3.000000 4.000000 7.000000 0.000000 1.000000 75% 8.000000 8.000000 10.000000 0.000000 2.000000 max 46.000000 21.000000 30.000000 6.000000 12.000000 Antigua and Barbuda Argentina Armenia Austria Azerbaijan count 438.000000 438.000000 438.000000 438.000000 438.000000 mean 0.063927 128.308219 8.200913 21.573059 8.394977 std 0.352208 198.186973 8.838277 32.526953 11.694105 min 0.000000 0.000000 0.000000 -1.000000 0.000000 25% 0.000000 9.250000 1.000000 0.000000 1.000000 50% 0.000000 92.500000 5.000000 6.000000 3.000000 75% 0.000000 193.250000 12.000000 29.750000 9.000000 max 5.000000 3351.000000 41.000000 218.000000 47.000000 ... United Kingdom.11 Uruguay Uzbekistan Vanuatu Venezuela count ... 438.000000 438.000000 438.000000 438.0 438.000000 mean ... 289.579909 2.513699 1.440639 0.0 3.794521 std ... 367.943940 5.032939 2.156336 0.0 3.475151 min ... 0.000000 0.000000 0.000000 0.0 -6.000000 25% ... 17.000000 0.000000 0.000000 0.0 0.000000 50% ... 130.500000 0.000000 0.000000 0.0 4.000000 75% ... 458.000000 2.000000 3.000000 0.0 6.000000 max ... 1820.000000 35.000000 8.000000 0.0 18.000000 Vietnam West Bank and Gaza Yemen Zambia Zimbabwe count 438.000000 438.000000 438.000000 438.000000 438.000000 mean 0.079909 6.178082 2.159817 2.785388 3.481735 std 0.412070 7.834400 4.925257 5.829740 8.703527 min -1.000000 0.000000 0.000000 0.000000 0.000000 25% 0.000000 0.000000 0.000000 0.000000 0.000000 50% 0.000000 3.000000 0.000000 0.000000 0.000000 75% 0.000000 9.000000 2.000000 3.000000 2.000000 max 3.000000 31.000000 52.000000 67.000000 70.000000 [8 rows x 189 columns] . world = pd.DataFrame({&quot;Country&quot;:[],&quot;Cases&quot;:[]}) world[&quot;Country&quot;] = df0.iloc[:,1:].columns cases = [] for i in world[&quot;Country&quot;]: cases.append(pd.to_numeric(df0[i][1:]).sum()) world[&quot;Cases&quot;]=cases country_list=list(world[&quot;Country&quot;].values) idx = 0 for i in country_list: sayac = 0 for j in i: if j==&quot;.&quot;: i = i[:sayac] country_list[idx]=i elif j==&quot;(&quot;: i = i[:sayac-1] country_list[idx]=i else: sayac += 1 idx += 1 world[&quot;Country&quot;]=country_list world = world.groupby(&quot;Country&quot;)[&quot;Cases&quot;].sum().reset_index() world.head() continent=pd.read_csv(&quot;./input/continents2.csv&quot;) continent[&quot;name&quot;]=continent[&quot;name&quot;].str.upper() continent[&quot;name&quot;] . 0 AFGHANISTAN 1 ÅLAND ISLANDS 2 ALBANIA 3 ALGERIA 4 AMERICAN SAMOA ... 244 WALLIS AND FUTUNA 245 WESTERN SAHARA 246 YEMEN 247 ZAMBIA 248 ZIMBABWE Name: name, Length: 249, dtype: object . world[&quot;Cases Range&quot;]=pd.cut(world[&quot;Cases&quot;],[-150000,50000,200000,800000,1500000,15000000],labels=[&quot;U50K&quot;,&quot;50Kto200K&quot;,&quot;200Kto800K&quot;,&quot;800Kto1.5M&quot;,&quot;1.5M+&quot;]) alpha =[] for i in world[&quot;Country&quot;].str.upper().values: if i == &quot;BRUNEI&quot;: i=&quot;BRUNEI DARUSSALAM&quot; elif i==&quot;US&quot;: i=&quot;UNITED STATES&quot; if len(continent[continent[&quot;name&quot;]==i][&quot;alpha-3&quot;].values)==0: alpha.append(np.nan) else: alpha.append(continent[continent[&quot;name&quot;]==i][&quot;alpha-3&quot;].values[0]) world[&quot;Alpha3&quot;]=alpha fig = px.choropleth(world.dropna(), locations=&quot;Alpha3&quot;, color=&quot;Cases Range&quot;, projection=&quot;mercator&quot;, color_discrete_sequence=[&quot;white&quot;,&quot;khaki&quot;,&quot;yellow&quot;,&quot;orange&quot;,&quot;red&quot;]) fig.update_geos(fitbounds=&quot;locations&quot;, visible=False) fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . count = [] for i in range(1,len(df_confirmed)): count.append(sum(pd.to_numeric(df_confirmed.iloc[i,1:].values))) df = pd.DataFrame() df[&quot;Date&quot;] = df_confirmed[&quot;Country/Region&quot;][1:] df[&quot;Cases&quot;] = count df=df.set_index(&quot;Date&quot;) count = [] for i in range(1,len(df_death)): count.append(sum(pd.to_numeric(df_death.iloc[i,1:].values))) df[&quot;Deaths&quot;] = count df.Cases.plot(title=&quot;Daily Covid19 Cases in World&quot;,marker=&quot;.&quot;,figsize=(10,5),label=&quot;daily cases&quot;) df.Cases.rolling(window=5).mean().plot(figsize=(10,5),label=&quot;MA5&quot;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Daily Covid19 Cases in World&#39;}, xlabel=&#39;Date&#39;&gt; . class Fbprophet(object): def fit(self,data): self.data = data self.model = Prophet(weekly_seasonality=True,daily_seasonality=False,yearly_seasonality=False) self.model.fit(self.data) def forecast(self,periods,freq): self.future = self.model.make_future_dataframe(periods=periods,freq=freq) self.df_forecast = self.model.predict(self.future) def plot(self,xlabel=&quot;Years&quot;,ylabel=&quot;Values&quot;): self.model.plot(self.df_forecast,xlabel=xlabel,ylabel=ylabel,figsize=(9,4)) self.model.plot_components(self.df_forecast,figsize=(9,6)) def R2(self): return r2_score(self.data.y, self.df_forecast.yhat[:len(df)]) df_fb = pd.DataFrame({&quot;ds&quot;:[],&quot;y&quot;:[]}) df_fb[&quot;ds&quot;] = pd.to_datetime(df.index) df_fb[&quot;y&quot;] = df.iloc[:,0].values model = Fbprophet() model.fit(df_fb) model.forecast(30,&quot;D&quot;) model.R2() forecast = model.df_forecast[[&quot;ds&quot;,&quot;yhat_lower&quot;,&quot;yhat_upper&quot;,&quot;yhat&quot;]].tail(30).reset_index().set_index(&quot;ds&quot;).drop(&quot;index&quot;,axis=1) forecast[&quot;yhat&quot;].plot(marker=&quot;.&quot;,figsize=(10,5)) plt.fill_between(x=forecast.index, y1=forecast[&quot;yhat_lower&quot;], y2=forecast[&quot;yhat_upper&quot;],color=&quot;gray&quot;) plt.legend([&quot;forecast&quot;,&quot;Bound&quot;],loc=&quot;upper left&quot;) plt.title(&quot;Forecasting of Next 30 Days Cases&quot;) plt.show() . US Cases Exploration . https://www.kaggle.com/chapagain/covid-19-usa-eda-chart-graph-map/execution . df_us_test = pd.read_csv(&quot;./input/us_covid19_daily.csv&quot;) df_us_states_test = pd.read_csv(&quot;./input/us_states_covid19_daily.csv&quot;) . df_cases_state = pd.read_csv(&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_state.csv&quot;) . state_map_dict = { &#39;AL&#39;: &#39;Alabama&#39;, &#39;AK&#39;: &#39;Alaska&#39;, &#39;AS&#39;: &#39;American Samoa&#39;, &#39;AZ&#39;: &#39;Arizona&#39;, &#39;AR&#39;: &#39;Arkansas&#39;, &#39;CA&#39;: &#39;California&#39;, &#39;CO&#39;: &#39;Colorado&#39;, &#39;CT&#39;: &#39;Connecticut&#39;, &#39;DE&#39;: &#39;Delaware&#39;, &#39;DC&#39;: &#39;District of Columbia&#39;, &#39;D.C.&#39;: &#39;District of Columbia&#39;, &#39;FM&#39;: &#39;Federated States of Micronesia&#39;, &#39;FL&#39;: &#39;Florida&#39;, &#39;GA&#39;: &#39;Georgia&#39;, &#39;GU&#39;: &#39;Guam&#39;, &#39;HI&#39;: &#39;Hawaii&#39;, &#39;ID&#39;: &#39;Idaho&#39;, &#39;IL&#39;: &#39;Illinois&#39;, &#39;IN&#39;: &#39;Indiana&#39;, &#39;IA&#39;: &#39;Iowa&#39;, &#39;KS&#39;: &#39;Kansas&#39;, &#39;KY&#39;: &#39;Kentucky&#39;, &#39;LA&#39;: &#39;Louisiana&#39;, &#39;ME&#39;: &#39;Maine&#39;, &#39;MH&#39;: &#39;Marshall Islands&#39;, &#39;MD&#39;: &#39;Maryland&#39;, &#39;MA&#39;: &#39;Massachusetts&#39;, &#39;MI&#39;: &#39;Michigan&#39;, &#39;MN&#39;: &#39;Minnesota&#39;, &#39;MS&#39;: &#39;Mississippi&#39;, &#39;MO&#39;: &#39;Missouri&#39;, &#39;MT&#39;: &#39;Montana&#39;, &#39;NE&#39;: &#39;Nebraska&#39;, &#39;NV&#39;: &#39;Nevada&#39;, &#39;NH&#39;: &#39;New Hampshire&#39;, &#39;NJ&#39;: &#39;New Jersey&#39;, &#39;NM&#39;: &#39;New Mexico&#39;, &#39;NY&#39;: &#39;New York&#39;, &#39;NC&#39;: &#39;North Carolina&#39;, &#39;ND&#39;: &#39;North Dakota&#39;, &#39;MP&#39;: &#39;Northern Mariana Islands&#39;, &#39;OH&#39;: &#39;Ohio&#39;, &#39;OK&#39;: &#39;Oklahoma&#39;, &#39;OR&#39;: &#39;Oregon&#39;, &#39;PW&#39;: &#39;Palau&#39;, &#39;PA&#39;: &#39;Pennsylvania&#39;, &#39;PR&#39;: &#39;Puerto Rico&#39;, &#39;RI&#39;: &#39;Rhode Island&#39;, &#39;SC&#39;: &#39;South Carolina&#39;, &#39;SD&#39;: &#39;South Dakota&#39;, &#39;TN&#39;: &#39;Tennessee&#39;, &#39;TX&#39;: &#39;Texas&#39;, &#39;UT&#39;: &#39;Utah&#39;, &#39;VT&#39;: &#39;Vermont&#39;, &#39;VI&#39;: &#39;Virgin Islands&#39;, &#39;VA&#39;: &#39;Virginia&#39;, &#39;WA&#39;: &#39;Washington&#39;, &#39;WV&#39;: &#39;West Virginia&#39;, &#39;WI&#39;: &#39;Wisconsin&#39;, &#39;WY&#39;: &#39;Wyoming&#39; } state_code_dict = {v:k for k, v in state_map_dict.items()} state_code_dict[&quot;Chicago&quot;] = &#39;Illinois&#39; def correct_state_names(x): try: return state_map_dict[x.split(&quot;,&quot;)[-1].strip()] except: return x.strip() def get_state_codes(x): try: return state_code_dict[x] except: return &quot;Others&quot; def get_state_name(x): try: for name, code in state_code_dict.items(): if code == x: return name return &#39;Others&#39; except: return &quot;Others&quot; . df_us_states = df_cases_state[df_cases_state[&#39;Country_Region&#39;] == &#39;US&#39;].copy() df_us_states[&quot;State_Code&quot;] = df_us_states[&quot;Province_State&quot;].apply(lambda x: get_state_codes(x)) . import plotly.graph_objects as go . fig = go.Figure(data=[go.Pie(labels=df_us_states[&#39;Province_State&#39;], values=df_us_states[&#39;Confirmed&#39;], hole=.35, textinfo=&#39;label+percent&#39; ) ]) fig.update_layout( title_text=&quot;US Confirmed Cases by States&quot;, # Add annotations in the center of the donut pies. annotations=[ dict(text=&#39;Confirmed&lt;br&gt;Cases&#39;, showarrow=False), ] ) fig.update_traces(textposition=&#39;inside&#39;) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . fig = go.Figure(data=[ go.Pie(labels=df_us_states[&#39;Province_State&#39;], values=df_us_states[&#39;Deaths&#39;], hole=.35, textinfo=&#39;label+percent&#39; ) ]) fig.update_layout( title_text=&quot;US Deaths Cases by States&quot;, # Add annotations in the center of the donut pies. annotations=[ dict(text=&#39;Deaths&lt;br&gt;Cases&#39;, showarrow=False), ] ) fig.update_traces(textposition=&#39;inside&#39;) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . df_confirmed_top = df_us_states.sort_values(&#39;Confirmed&#39;, ascending=False).head(10) fig = go.Figure(data=[ go.Bar(name=&#39;Confirmed&#39;, x=df_confirmed_top[&#39;Province_State&#39;], y=df_confirmed_top[&#39;Confirmed&#39;], text=df_confirmed_top[&#39;Confirmed&#39;], texttemplate=&#39;%{text:.2s}&#39;, textposition=&#39;outside&#39;), go.Bar(name=&#39;Deaths&#39;, x=df_confirmed_top[&#39;Province_State&#39;], y=df_confirmed_top[&#39;Deaths&#39;], text=df_confirmed_top[&#39;Deaths&#39;], texttemplate=&#39;%{text:.2s}&#39;, textposition=&#39;outside&#39;), go.Bar(name=&#39;Recovered&#39;, x=df_confirmed_top[&#39;Province_State&#39;], y=df_confirmed_top[&#39;Recovered&#39;], text=df_confirmed_top[&#39;Recovered&#39;], texttemplate=&#39;%{text:.2s}&#39;, textposition=&#39;outside&#39;), #go.Bar(name=&#39;People Tested&#39;, x=df_confirmed_top[&#39;Province_State&#39;], y=df_confirmed_top[&#39;People_Tested&#39;], # text=df_confirmed_top[&#39;People_Tested&#39;], texttemplate=&#39;%{text:.2s}&#39;, textposition=&#39;outside&#39;), ]) # Change the bar mode fig.update_layout( title_text=&quot;Top 10 US States with Confirmed Cases&quot;, barmode=&#39;group&#39;, #legend_orientation=&quot;h&quot;, yaxis_type=&#39;log&#39;, yaxis_title=&#39;Cases Count in Log Scale&#39; ) fig.update_layout(legend_orientation=&quot;h&quot;, legend=dict(x=0, y=1.1)) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . df_us_states . Province_State Country_Region Last_Update Lat Long_ Confirmed Deaths Recovered Active FIPS Incident_Rate People_Tested People_Hospitalized Mortality_Rate UID ISO3 Testing_Rate Hospitalization_Rate State_Code . 492 Alabama | US | 2021-04-11 18:31:04 | 32.3182 | -86.9023 | 518899 | 10712 | NaN | NaN | 1.0 | 10582.896627 | 2436418.0 | NaN | 2.064371 | 84000001.0 | USA | 49690.517490 | NaN | AL | . 493 Alaska | US | 2021-04-11 18:31:04 | 61.3707 | -152.4044 | 64782 | 313 | NaN | NaN | 2.0 | 8855.504446 | 1966048.0 | NaN | 0.483159 | 84000002.0 | USA | 268752.845006 | NaN | AK | . 494 American Samoa | US | 2021-04-11 18:31:04 | -14.2710 | -170.1320 | 0 | 0 | NaN | NaN | 60.0 | 0.000000 | 2140.0 | NaN | NaN | 16.0 | ASM | 3846.084722 | NaN | AS | . 495 Arizona | US | 2021-04-11 18:31:04 | 33.7298 | -111.4312 | 849021 | 17092 | NaN | NaN | 4.0 | 11664.432070 | 8761941.0 | NaN | 2.013142 | 84000004.0 | USA | 120377.547307 | NaN | AZ | . 496 Arkansas | US | 2021-04-11 18:31:04 | 34.9697 | -92.3731 | 332053 | 5661 | NaN | NaN | 5.0 | 11003.133404 | 2898574.0 | NaN | 1.704848 | 84000005.0 | USA | 96049.113859 | NaN | AR | . 497 California | US | 2021-04-11 18:31:04 | 36.1162 | -119.6816 | 3698160 | 60444 | NaN | NaN | 6.0 | 9359.534137 | 56078860.0 | NaN | 1.634434 | 84000006.0 | USA | 141927.878874 | NaN | CA | . 498 Colorado | US | 2021-04-11 18:31:04 | 39.0598 | -105.3111 | 477443 | 6155 | NaN | NaN | 8.0 | 8290.760334 | 7269129.0 | NaN | 1.289159 | 84000008.0 | USA | 126227.856252 | NaN | CO | . 499 Connecticut | US | 2021-04-11 18:31:04 | 41.5978 | -72.7554 | 321586 | 7944 | NaN | NaN | 9.0 | 9019.919014 | 7476439.0 | NaN | 2.470257 | 84000009.0 | USA | 209700.902059 | NaN | CT | . 500 Delaware | US | 2021-04-11 18:31:04 | 39.3185 | -75.5071 | 98570 | 1578 | NaN | NaN | 10.0 | 10122.575901 | 1620589.0 | NaN | 1.600893 | 84000010.0 | USA | 166425.232397 | NaN | DE | . 501 Diamond Princess | US | 2021-04-11 18:31:04 | NaN | NaN | 49 | 0 | NaN | NaN | 88888.0 | NaN | NaN | NaN | 0.000000 | 84088888.0 | USA | NaN | NaN | Others | . 502 District of Columbia | US | 2021-04-11 18:31:04 | 38.8974 | -77.0268 | 45830 | 1081 | NaN | NaN | 11.0 | 6493.810122 | 1423475.0 | NaN | 2.358717 | 84000011.0 | USA | 201697.062270 | NaN | D.C. | . 503 Florida | US | 2021-04-11 18:31:04 | 27.7663 | -81.6868 | 2124068 | 34039 | NaN | NaN | 12.0 | 9889.626640 | 21476893.0 | NaN | 1.602538 | 84000012.0 | USA | 99996.070349 | NaN | FL | . 504 Georgia | US | 2021-04-11 18:31:04 | 33.0406 | -83.6431 | 1072700 | 19488 | NaN | NaN | 13.0 | 10103.204893 | 7975847.0 | NaN | 1.816724 | 84000013.0 | USA | 75120.365836 | NaN | GA | . 505 Grand Princess | US | 2021-04-11 18:31:04 | NaN | NaN | 103 | 3 | NaN | NaN | 99999.0 | NaN | NaN | NaN | 2.912621 | 84099999.0 | USA | NaN | NaN | Others | . 506 Guam | US | 2021-04-11 18:31:04 | 13.4443 | 144.7937 | 7833 | 136 | NaN | NaN | 66.0 | 4769.559578 | 134339.0 | NaN | 1.736244 | 316.0 | GUM | 81799.803932 | NaN | GU | . 507 Hawaii | US | 2021-04-11 18:31:04 | 21.0943 | -157.4983 | 31975 | 471 | NaN | NaN | 15.0 | 2258.325611 | 1328203.0 | NaN | 1.473026 | 84000015.0 | USA | 93808.126723 | NaN | HI | . 508 Idaho | US | 2021-04-11 18:31:04 | 44.2405 | -114.4788 | 183153 | 1993 | NaN | NaN | 16.0 | 10248.815796 | 685952.0 | NaN | 1.088161 | 84000016.0 | USA | 38384.278132 | NaN | ID | . 509 Illinois | US | 2021-04-11 18:31:04 | 40.3495 | -88.9861 | 1279665 | 23793 | NaN | NaN | 17.0 | 10098.509125 | 21102407.0 | NaN | 1.859315 | 84000017.0 | USA | 166530.185362 | NaN | IL | . 510 Indiana | US | 2021-04-11 18:31:04 | 39.8494 | -86.2583 | 698692 | 13148 | NaN | NaN | 18.0 | 10378.331424 | 9187249.0 | NaN | 1.881802 | 84000018.0 | USA | 136466.876672 | NaN | IN | . 511 Iowa | US | 2021-04-11 18:31:04 | 42.0115 | -93.2105 | 356893 | 5857 | NaN | NaN | 19.0 | 11311.730009 | 1393075.0 | NaN | 1.641108 | 84000019.0 | USA | 44153.537006 | NaN | IA | . 512 Kansas | US | 2021-04-11 18:31:04 | 38.5266 | -96.7265 | 305773 | 4895 | NaN | NaN | 20.0 | 10495.710383 | 1335069.0 | NaN | 1.600861 | 84000020.0 | USA | 45826.471160 | NaN | KS | . 513 Kentucky | US | 2021-04-11 18:31:04 | 37.6681 | -84.6701 | 432720 | 6241 | NaN | NaN | 21.0 | 9685.579048 | 4393731.0 | NaN | 1.442272 | 84000021.0 | USA | 98344.954969 | NaN | KY | . 514 Louisiana | US | 2021-04-11 18:31:04 | 31.1695 | -91.8678 | 448838 | 10216 | NaN | NaN | 22.0 | 9654.934161 | 6186986.0 | NaN | 2.276100 | 84000022.0 | USA | 133087.979377 | NaN | LA | . 515 Maine | US | 2021-04-11 18:31:04 | 44.6939 | -69.3819 | 53960 | 750 | NaN | NaN | 23.0 | 4014.247753 | 1872183.0 | NaN | 1.389918 | 84000023.0 | USA | 139277.361012 | NaN | ME | . 516 Maryland | US | 2021-04-11 18:31:04 | 39.0639 | -76.8021 | 426730 | 8455 | NaN | NaN | 24.0 | 7058.428498 | 9090254.0 | NaN | 1.981347 | 84000024.0 | USA | 150359.496368 | NaN | MD | . 517 Massachusetts | US | 2021-04-11 18:31:04 | 42.2302 | -71.5301 | 657578 | 17379 | NaN | NaN | 25.0 | 9540.481883 | 19748011.0 | NaN | 2.642880 | 84000025.0 | USA | 286514.362054 | NaN | MA | . 518 Michigan | US | 2021-04-11 18:31:04 | 43.3266 | -84.5361 | 820404 | 17563 | NaN | NaN | 26.0 | 8214.836760 | 11938983.0 | NaN | 2.140775 | 84000026.0 | USA | 119546.950557 | NaN | MI | . 519 Minnesota | US | 2021-04-11 18:31:04 | 45.6945 | -93.9002 | 542053 | 7035 | NaN | NaN | 27.0 | 9611.495927 | 7978553.0 | NaN | 1.297844 | 84000027.0 | USA | 141472.936532 | NaN | MN | . 520 Mississippi | US | 2021-04-11 18:31:04 | 32.7416 | -89.6787 | 307332 | 7095 | NaN | NaN | 28.0 | 10326.499110 | 1845873.0 | NaN | 2.308578 | 84000028.0 | USA | 62022.197141 | NaN | MS | . 521 Missouri | US | 2021-04-11 18:31:04 | 38.4561 | -92.2884 | 588549 | 8884 | NaN | NaN | 29.0 | 9589.505571 | 5027327.0 | NaN | 1.509475 | 84000029.0 | USA | 81912.602478 | NaN | MO | . 522 Montana | US | 2021-04-11 18:31:04 | 46.9219 | -110.4544 | 106183 | 1523 | NaN | NaN | 30.0 | 9934.991177 | 1245182.0 | NaN | 1.434316 | 84000030.0 | USA | 116505.205010 | NaN | MT | . 523 Nebraska | US | 2021-04-11 18:31:04 | 41.1254 | -98.2681 | 214207 | 2220 | NaN | NaN | 31.0 | 11073.517066 | 2695720.0 | NaN | 1.036381 | 84000031.0 | USA | 139356.330205 | NaN | NE | . 524 Nevada | US | 2021-04-11 18:31:04 | 38.3135 | -117.0554 | 308024 | 5332 | NaN | NaN | 32.0 | 10000.272713 | 3043137.0 | NaN | 1.731034 | 84000032.0 | USA | 98798.145289 | NaN | NV | . 525 New Hampshire | US | 2021-04-11 18:31:04 | 43.4525 | -71.5639 | 88446 | 1256 | NaN | NaN | 33.0 | 6504.764615 | 1728015.0 | NaN | 1.420076 | 84000033.0 | USA | 127086.932444 | NaN | NH | . 526 New Jersey | US | 2021-04-11 18:31:04 | 40.2989 | -74.5210 | 953490 | 24870 | NaN | NaN | 34.0 | 10734.852553 | 12694233.0 | NaN | 2.608313 | 84000034.0 | USA | 142917.827698 | NaN | NJ | . 527 New Mexico | US | 2021-04-11 18:31:04 | 34.8405 | -106.2485 | 193368 | 3976 | NaN | NaN | 35.0 | 9221.925107 | 3079551.0 | NaN | 2.056183 | 84000035.0 | USA | 146867.054967 | NaN | NM | . 528 New York | US | 2021-04-11 18:31:04 | 42.1657 | -74.9481 | 1953953 | 51056 | NaN | NaN | 36.0 | 10044.191909 | 47244227.0 | NaN | 2.612959 | 84000036.0 | USA | 242856.446694 | NaN | NY | . 529 North Carolina | US | 2021-04-11 18:31:04 | 35.6301 | -79.8064 | 929406 | 12248 | NaN | NaN | 37.0 | 8861.542299 | 10765561.0 | NaN | 1.317831 | 84000037.0 | USA | 102645.640519 | NaN | NC | . 530 North Dakota | US | 2021-04-11 18:31:04 | 47.5289 | -99.7840 | 104745 | 1502 | NaN | NaN | 38.0 | 13744.944637 | 1485211.0 | NaN | 1.433959 | 84000038.0 | USA | 194893.722558 | NaN | ND | . 531 Northern Mariana Islands | US | 2021-04-11 18:31:04 | 15.0979 | 145.6739 | 160 | 2 | NaN | NaN | 69.0 | 290.149427 | 17542.0 | NaN | 1.250000 | 580.0 | MNP | 31811.257798 | NaN | MP | . 532 Ohio | US | 2021-04-11 18:31:04 | 40.3888 | -82.7649 | 1037600 | 18827 | NaN | NaN | 39.0 | 8876.645764 | 11310109.0 | NaN | 1.814476 | 84000039.0 | USA | 96757.740117 | NaN | OH | . 533 Oklahoma | US | 2021-04-11 18:31:04 | 35.5653 | -96.9289 | 442805 | 6669 | NaN | NaN | 40.0 | 11190.504050 | 3838783.0 | NaN | 1.506081 | 84000040.0 | USA | 97013.169922 | NaN | OK | . 534 Oregon | US | 2021-04-11 18:31:04 | 44.5720 | -122.0709 | 170085 | 2440 | NaN | NaN | 41.0 | 4032.612749 | 4319403.0 | NaN | 1.434577 | 84000041.0 | USA | 102410.439532 | NaN | OR | . 535 Pennsylvania | US | 2021-04-11 18:31:04 | 40.5908 | -77.2098 | 1073924 | 25393 | NaN | NaN | 42.0 | 8388.727720 | 12148589.0 | NaN | 2.364506 | 84000042.0 | USA | 94896.105597 | NaN | PA | . 536 Puerto Rico | US | 2021-04-11 18:31:04 | 18.2208 | -66.5901 | 114197 | 2154 | NaN | NaN | 72.0 | 3575.702619 | 415664.0 | NaN | 1.886214 | 630.0 | PRI | 13015.147976 | NaN | PR | . 537 Rhode Island | US | 2021-04-11 18:31:04 | 41.6809 | -71.5118 | 141097 | 2638 | NaN | NaN | 44.0 | 13319.066871 | 3680071.0 | NaN | 1.869636 | 84000044.0 | USA | 347385.924156 | NaN | RI | . 538 South Carolina | US | 2021-04-11 18:31:04 | 33.8569 | -80.9450 | 562691 | 9276 | NaN | NaN | 45.0 | 10928.767844 | 5822980.0 | NaN | 1.648507 | 84000045.0 | USA | 113095.813828 | NaN | SC | . 539 South Dakota | US | 2021-04-11 18:31:04 | 44.2998 | -99.4388 | 119705 | 1946 | NaN | NaN | 46.0 | 13531.202418 | 751549.0 | NaN | 1.625663 | 84000046.0 | USA | 84953.524465 | NaN | SD | . 540 Tennessee | US | 2021-04-11 18:31:04 | 35.7478 | -86.6923 | 822085 | 12001 | NaN | NaN | 47.0 | 12037.839422 | 7346127.0 | NaN | 1.459825 | 84000047.0 | USA | 107569.773446 | NaN | TN | . 541 Texas | US | 2021-04-11 18:31:04 | 31.0545 | -97.5635 | 2825675 | 49188 | NaN | NaN | 48.0 | 9745.091036 | 21970307.0 | NaN | 1.740752 | 84000048.0 | USA | 75770.441326 | NaN | TX | . 542 Utah | US | 2021-04-11 18:31:04 | 40.1500 | -111.8624 | 390104 | 2159 | NaN | NaN | 49.0 | 12168.094529 | 3151927.0 | NaN | 0.553442 | 84000049.0 | USA | 98314.669125 | NaN | UT | . 543 Vermont | US | 2021-04-11 18:31:04 | 44.0459 | -72.7107 | 21202 | 231 | NaN | NaN | 50.0 | 3397.816308 | 1425517.0 | NaN | 1.089520 | 84000050.0 | USA | 228452.264383 | NaN | VT | . 544 Virgin Islands | US | 2021-04-11 18:31:04 | 18.3358 | -64.8963 | 2971 | 26 | NaN | NaN | 78.0 | 2769.698326 | 90048.0 | NaN | 0.875126 | 850.0 | VIR | 83946.750196 | NaN | VI | . 545 Virginia | US | 2021-04-11 18:31:04 | 37.7693 | -78.1700 | 635552 | 10472 | NaN | NaN | 51.0 | 7445.967843 | 6680238.0 | NaN | 1.647702 | 84000051.0 | USA | 78263.993086 | NaN | VA | . 546 Washington | US | 2021-04-11 18:31:04 | 47.4009 | -121.4905 | 376230 | 5322 | NaN | NaN | 53.0 | 4940.712890 | 6115030.0 | NaN | 1.414560 | 84000053.0 | USA | 80303.557778 | NaN | WA | . 547 West Virginia | US | 2021-04-11 18:31:04 | 38.4912 | -80.9545 | 146169 | 2745 | NaN | NaN | 54.0 | 8156.083178 | 2571730.0 | NaN | 1.877963 | 84000054.0 | USA | 143499.947270 | NaN | WV | . 548 Wisconsin | US | 2021-04-11 18:31:04 | 44.2685 | -89.6165 | 644901 | 7379 | NaN | NaN | 55.0 | 11076.141009 | 7847462.0 | NaN | 1.144207 | 84000055.0 | USA | 134779.750187 | NaN | WI | . 549 Wyoming | US | 2021-04-11 18:31:04 | 42.7560 | -107.3025 | 56873 | 701 | NaN | NaN | 56.0 | 9826.715438 | 698827.0 | NaN | 1.232571 | 84000056.0 | USA | 120745.768100 | NaN | WY | . df_us_states_test[&quot;state_name&quot;] = df_us_states_test[&quot;state&quot;].apply(lambda x: get_state_name(x)) df_us_states_test.head(2) . date state positive probableCases negative pending totalTestResultsSource totalTestResults hospitalizedCurrently hospitalizedCumulative ... deathIncrease hospitalizedIncrease hash commercialScore negativeRegularScore negativeScore positiveScore score grade state_name . 15632 2020-01-22 | WA | 0.0 | NaN | 0.0 | NaN | totalTestEncountersViral | 0.0 | NaN | NaN | ... | 0 | 0 | 4502685e582e68071bfe8f7e1b307bc09c16728d | 0 | 0 | 0 | 0 | 0 | NaN | Washington | . 15631 2020-01-22 | MA | NaN | NaN | NaN | NaN | totalTestsViral | 1.0 | NaN | NaN | ... | 0 | 0 | 01f5dcd6631859503ef1b62d81d49e41d12fc1bd | 0 | 0 | 0 | 0 | 0 | NaN | Massachusetts | . 2 rows × 56 columns . df_t = df_us_states_test.groupby(&#39;date&#39;).sum().reset_index() df_t . date positive probableCases negative pending totalTestResults hospitalizedCurrently hospitalizedCumulative inIcuCurrently inIcuCumulative ... totalTestResultsIncrease posNeg deathIncrease hospitalizedIncrease commercialScore negativeRegularScore negativeScore positiveScore score grade . 0 2020-01-22 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | . 1 2020-01-23 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | . 2 2020-01-24 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | . 3 2020-01-25 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | . 4 2020-01-26 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 315 2020-12-02 | 13711156.0 | 530350.0 | 156787587.0 | 14368.0 | 196576482.0 | 100322.0 | 570121.0 | 19680.0 | 31038.0 | ... | 1459202 | 170498743 | 2733 | 5028 | 0 | 0 | 0 | 0 | 0 | 0.0 | . 316 2020-12-03 | 13921360.0 | 545544.0 | 158026052.0 | 15106.0 | 198404712.0 | 100755.0 | 575452.0 | 19723.0 | 31276.0 | ... | 1828230 | 171947412 | 2706 | 5331 | 0 | 0 | 0 | 0 | 0 | 0.0 | . 317 2020-12-04 | 14146191.0 | 560291.0 | 159286709.0 | 12714.0 | 200259581.0 | 101276.0 | 580104.0 | 19858.0 | 31608.0 | ... | 1854869 | 173432900 | 2563 | 4652 | 0 | 0 | 0 | 0 | 0 | 0.0 | . 318 2020-12-05 | 14357264.0 | 574764.0 | 160813704.0 | 13433.0 | 202429337.0 | 101190.0 | 583420.0 | 19950.0 | 31831.0 | ... | 2169756 | 175170968 | 2445 | 3316 | 0 | 0 | 0 | 0 | 0 | 0.0 | . 319 2020-12-06 | 14534035.0 | 583908.0 | 161986294.0 | 13592.0 | 204063869.0 | 101487.0 | 585676.0 | 20145.0 | 31946.0 | ... | 1634532 | 176520329 | 1138 | 2256 | 0 | 0 | 0 | 0 | 0 | 0.0 | . 320 rows × 47 columns . fig = go.Figure() fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;positive&#39;], mode=&#39;lines&#39;, name=&#39;Confirmed&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;death&#39;], mode=&#39;lines&#39;, name=&#39;Deaths&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;recovered&#39;], mode=&#39;lines&#39;, name=&#39;Recovered&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;hospitalized&#39;], mode=&#39;lines&#39;, name=&#39;Hospitalized&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;total&#39;], mode=&#39;lines&#39;, name=&#39;Total Tested&#39;)) fig.update_layout( xaxis_title=&quot;&quot;, yaxis_title=&quot;Cases Count in Log Scale&quot;, title = &#39;Time Series - Confirmed, Deaths &amp; Recovered Cases in USA&#39;, yaxis_type=&#39;log&#39; ) fig.update_layout( legend=dict( x=0, y=1, traceorder=&quot;normal&quot;, #bgcolor=&quot;LightSteelBlue&quot;, bordercolor=&quot;silver&quot;, borderwidth=1 ) ) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . fig = go.Figure() fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;recovered&#39;]/df_t[&#39;positive&#39;]*100, mode=&#39;lines&#39;, name=&#39;Recovery Rate&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;death&#39;]/df_t[&#39;positive&#39;]*100, mode=&#39;lines&#39;, name=&#39;Death Rate&#39;)) fig.update_layout( xaxis_title=&quot;&quot;, yaxis_title=&quot;Recovery/Death Rate Percentage (%)&quot;, title = &#39;Time Series - Recovery and Death Rate in USA&#39;, yaxis_type=&#39;log&#39; ) fig.update_layout( legend=dict( x=0, y=1, traceorder=&quot;normal&quot;, #font=dict( # family=&quot;sans-serif&quot;, # size=12, # color=&quot;black&quot; #), #bgcolor=&quot;LightSteelBlue&quot;, bordercolor=&quot;silver&quot;, borderwidth=1 ) ) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . df_t = df_us_states_test.groupby([&#39;state_name&#39;, &#39;date&#39;]).sum().reset_index() df_t[&quot;date_reported&quot;] = pd.to_datetime(df_t[&quot;date&quot;]).dt.strftime(&#39;%m/%d/%Y&#39;) df_t[&#39;state&#39;] = df_t[&#39;state_name&#39;].apply(lambda x: get_state_codes(x)) # while calculating mortality rate, adding 1 to confirmed to avoid divide by zero df_t[&#39;mortality_rate&#39;] = df_t[&#39;death&#39;] / (df_t[&#39;positive&#39;]+1) * 100 df_t2 = df_t.groupby([&#39;date&#39;, &#39;state_name&#39;]).max().reset_index() df_t2.sort_values(&#39;date&#39;, ascending=False).head() . date state_name positive probableCases negative pending totalTestResults hospitalizedCurrently hospitalizedCumulative inIcuCurrently ... hospitalizedIncrease commercialScore negativeRegularScore negativeScore positiveScore score grade date_reported state mortality_rate . 15632 2020-12-06 | Wyoming | 36317.0 | 4756.0 | 144576.0 | 0.0 | 417627.0 | 212.0 | 852.0 | 0.0 | ... | 9 | 0 | 0 | 0 | 0 | 0 | 0.0 | 12/06/2020 | WY | 0.707638 | . 15604 2020-12-06 | Montana | 67875.0 | 0.0 | 620339.0 | 0.0 | 688214.0 | 475.0 | 2836.0 | 77.0 | ... | 21 | 0 | 0 | 0 | 0 | 0 | 0.0 | 12/06/2020 | MT | 1.084330 | . 15602 2020-12-06 | Mississippi | 164931.0 | 37611.0 | 982395.0 | 0.0 | 1147326.0 | 1157.0 | 7486.0 | 286.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 12/06/2020 | MS | 2.401596 | . 15601 2020-12-06 | Minnesota | 350862.0 | 8459.0 | 2301135.0 | 0.0 | 4431677.0 | 1679.0 | 18233.0 | 367.0 | ... | 174 | 0 | 0 | 0 | 0 | 0 | 0.0 | 12/06/2020 | MN | 1.135486 | . 15600 2020-12-06 | Michigan | 426576.0 | 31540.0 | 6538482.0 | 0.0 | 6965058.0 | 4141.0 | 0.0 | 855.0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0.0 | 12/06/2020 | MI | 2.419493 | . 5 rows × 51 columns . fig = px.choropleth(df_t2, locations=&quot;state&quot;, locationmode=&#39;USA-states&#39;, scope=&quot;usa&quot;, hover_name=&quot;state_name&quot;, hover_data=[&quot;positive&quot;, &quot;death&quot;, &quot;recovered&quot;], animation_frame=&quot;date&quot;, color=np.log(df_t2[&quot;positive&quot;]+1), title=&quot;US COVID-19 Progression Animation Over Time&quot;, color_continuous_scale=px.colors.sequential.Plasma, ) #fig.update_coloraxes(colorscale=&quot;hot&quot;) #fig.update(layout_coloraxis_showscale=False) fig.update_coloraxes(colorbar_title=&quot;Color&lt;br&gt;Confirmed Cases&lt;br&gt;in Log Scale&quot;) fig.show() .",
            "url": "https://chenghsu.github.io/Data-Analysis-Blog/2021/04/11/covid_prediction.html",
            "relUrl": "/2021/04/11/covid_prediction.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Covid Prediction",
            "content": "import warnings warnings.filterwarnings(&#39;ignore&#39;) import pandas as pd import numpy as np import matplotlib.pyplot as plt import plotly.express as px from fbprophet import Prophet from sklearn.metrics import r2_score plt.style.use(&quot;ggplot&quot;) df0 = pd.read_csv(&quot;./input/CONVENIENT_global_confirmed_cases.csv&quot;) df1 = pd.read_csv(&quot;./input/CONVENIENT_global_deaths.csv&quot;) df2 = pd.read_csv(&quot;./input/CONVENIENT_us_confirmed_cases.csv&quot;) df3 = pd.read_csv(&quot;./input/CONVENIENT_us_deaths.csv&quot;) . df0 . Country/Region Afghanistan Albania Algeria Andorra Angola Antigua and Barbuda Argentina Armenia Australia ... United Kingdom.11 Uruguay Uzbekistan Vanuatu Venezuela Vietnam West Bank and Gaza Yemen Zambia Zimbabwe . 0 Province/State | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Australian Capital Territory | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 1/23/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 1/24/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 1/25/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 1/26/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 434 3/31/21 | 70.0 | 434.0 | 131.0 | 66.0 | 129.0 | 0.0 | 16056.0 | 1148.0 | 0.0 | ... | 4052.0 | 3088.0 | 187.0 | 0.0 | 1348.0 | 9.0 | 2288.0 | 110.0 | 219.0 | 43.0 | . 435 4/1/21 | 63.0 | 349.0 | 112.0 | 43.0 | 88.0 | 11.0 | 14430.0 | 1097.0 | 0.0 | ... | 4478.0 | 2639.0 | 181.0 | 0.0 | 1254.0 | 14.0 | 2292.0 | 174.0 | 131.0 | 14.0 | . 436 4/2/21 | 55.0 | 336.0 | 125.0 | 62.0 | 68.0 | 5.0 | 9902.0 | 1116.0 | 0.0 | ... | 3402.0 | 3380.0 | 189.0 | 0.0 | 979.0 | 3.0 | 2248.0 | 89.0 | 181.0 | 7.0 | . 437 4/3/21 | 23.0 | 341.0 | 95.0 | 59.0 | 112.0 | 18.0 | 10384.0 | 1192.0 | 0.0 | ... | 3423.0 | 2336.0 | 196.0 | 0.0 | 1607.0 | 6.0 | 1589.0 | 77.0 | 70.0 | 8.0 | . 438 4/4/21 | 81.0 | 348.0 | 98.0 | 57.0 | 52.0 | 0.0 | 9955.0 | 590.0 | 0.0 | ... | 2297.0 | 3853.0 | 188.0 | 0.0 | 1786.0 | 5.0 | 2806.0 | 101.0 | 130.0 | 12.0 | . 439 rows × 275 columns . df1 . Country/Region Afghanistan Albania Algeria Andorra Angola Antigua and Barbuda Argentina Armenia Australia ... United Kingdom.11 Uruguay Uzbekistan Vanuatu Venezuela Vietnam West Bank and Gaza Yemen Zambia Zimbabwe . 0 Province/State | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Australian Capital Territory | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 1/23/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 1/24/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 1/25/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 1/26/20 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 434 3/31/21 | 8.0 | 8.0 | 4.0 | 0.0 | 1.0 | 0.0 | 122.0 | 18.0 | 0.0 | ... | 43.0 | 21.0 | 1.0 | 0.0 | 13.0 | 0.0 | 13.0 | 6.0 | 6.0 | 3.0 | . 435 4/1/21 | 5.0 | 6.0 | 3.0 | 0.0 | 1.0 | 0.0 | 83.0 | 18.0 | 0.0 | ... | 51.0 | 35.0 | 1.0 | 0.0 | 13.0 | 0.0 | 18.0 | 18.0 | 4.0 | 0.0 | . 436 4/2/21 | 6.0 | 6.0 | 3.0 | 1.0 | 0.0 | 0.0 | 82.0 | 19.0 | 0.0 | ... | 52.0 | 32.0 | 0.0 | 0.0 | 14.0 | 0.0 | 16.0 | 10.0 | 3.0 | 1.0 | . 437 4/3/21 | 1.0 | 9.0 | 3.0 | 1.0 | 2.0 | 0.0 | 83.0 | 23.0 | 0.0 | ... | 10.0 | 30.0 | 0.0 | 0.0 | 18.0 | 0.0 | 20.0 | 16.0 | 0.0 | 0.0 | . 438 4/4/21 | 1.0 | 9.0 | 3.0 | 0.0 | 2.0 | 0.0 | 93.0 | 17.0 | 0.0 | ... | 10.0 | 30.0 | 1.0 | 0.0 | 15.0 | 0.0 | 25.0 | 14.0 | 5.0 | 1.0 | . 439 rows × 275 columns . world = pd.DataFrame({&quot;Country&quot;:[],&quot;Cases&quot;:[]}) world[&quot;Country&quot;] = df_confirmed.iloc[:,1:].columns #world[&quot;Country&quot;] = df_confirmed.iloc[:1,1:].keys() cases = [] world . NameError Traceback (most recent call last) &lt;ipython-input-6-5af7a718ee8e&gt; in &lt;module&gt; 1 world = pd.DataFrame({&#34;Country&#34;:[],&#34;Cases&#34;:[]}) -&gt; 2 world[&#34;Country&#34;] = df_confirmed.iloc[:,1:].columns 3 #world[&#34;Country&#34;] = df_confirmed.iloc[:1,1:].keys() 4 cases = [] 5 world NameError: name &#39;df_confirmed&#39; is not defined . print(df0.info()) print(df1.info()) . print(df0.describe()) print(df1.describe()) . world = pd.DataFrame({&quot;Country&quot;:[],&quot;Cases&quot;:[]}) world[&quot;Country&quot;] = df0.iloc[:,1:].columns cases = [] for i in world[&quot;Country&quot;]: cases.append(pd.to_numeric(df0[i][1:]).sum()) world[&quot;Cases&quot;]=cases country_list=list(world[&quot;Country&quot;].values) idx = 0 for i in country_list: sayac = 0 for j in i: if j==&quot;.&quot;: i = i[:sayac] country_list[idx]=i elif j==&quot;(&quot;: i = i[:sayac-1] country_list[idx]=i else: sayac += 1 idx += 1 world[&quot;Country&quot;]=country_list world = world.groupby(&quot;Country&quot;)[&quot;Cases&quot;].sum().reset_index() world.head() continent=pd.read_csv(&quot;./input/continents2.csv&quot;) continent[&quot;name&quot;]=continent[&quot;name&quot;].str.upper() continent[&quot;name&quot;] . world[&quot;Cases Range&quot;]=pd.cut(world[&quot;Cases&quot;],[-150000,50000,200000,800000,1500000,15000000],labels=[&quot;U50K&quot;,&quot;50Kto200K&quot;,&quot;200Kto800K&quot;,&quot;800Kto1.5M&quot;,&quot;1.5M+&quot;]) alpha =[] for i in world[&quot;Country&quot;].str.upper().values: if i == &quot;BRUNEI&quot;: i=&quot;BRUNEI DARUSSALAM&quot; elif i==&quot;US&quot;: i=&quot;UNITED STATES&quot; if len(continent[continent[&quot;name&quot;]==i][&quot;alpha-3&quot;].values)==0: alpha.append(np.nan) else: alpha.append(continent[continent[&quot;name&quot;]==i][&quot;alpha-3&quot;].values[0]) world[&quot;Alpha3&quot;]=alpha fig = px.choropleth(world.dropna(), locations=&quot;Alpha3&quot;, color=&quot;Cases Range&quot;, projection=&quot;mercator&quot;, color_discrete_sequence=[&quot;white&quot;,&quot;khaki&quot;,&quot;yellow&quot;,&quot;orange&quot;,&quot;red&quot;]) fig.update_geos(fitbounds=&quot;locations&quot;, visible=False) fig.update_layout(margin={&quot;r&quot;:0,&quot;t&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . count = [] for i in range(1,len(df_confirmed)): count.append(sum(pd.to_numeric(df_confirmed.iloc[i,1:].values))) df = pd.DataFrame() df[&quot;Date&quot;] = df_confirmed[&quot;Country/Region&quot;][1:] df[&quot;Cases&quot;] = count df=df.set_index(&quot;Date&quot;) count = [] for i in range(1,len(df_death)): count.append(sum(pd.to_numeric(df_death.iloc[i,1:].values))) df[&quot;Deaths&quot;] = count df.Cases.plot(title=&quot;Daily Covid19 Cases in World&quot;,marker=&quot;.&quot;,figsize=(10,5),label=&quot;daily cases&quot;) df.Cases.rolling(window=5).mean().plot(figsize=(10,5),label=&quot;MA5&quot;) . class Fbprophet(object): def fit(self,data): self.data = data self.model = Prophet(weekly_seasonality=True,daily_seasonality=False,yearly_seasonality=False) self.model.fit(self.data) def forecast(self,periods,freq): self.future = self.model.make_future_dataframe(periods=periods,freq=freq) self.df_forecast = self.model.predict(self.future) def plot(self,xlabel=&quot;Years&quot;,ylabel=&quot;Values&quot;): self.model.plot(self.df_forecast,xlabel=xlabel,ylabel=ylabel,figsize=(9,4)) self.model.plot_components(self.df_forecast,figsize=(9,6)) def R2(self): return r2_score(self.data.y, self.df_forecast.yhat[:len(df)]) df_fb = pd.DataFrame({&quot;ds&quot;:[],&quot;y&quot;:[]}) df_fb[&quot;ds&quot;] = pd.to_datetime(df.index) df_fb[&quot;y&quot;] = df.iloc[:,0].values model = Fbprophet() model.fit(df_fb) model.forecast(30,&quot;D&quot;) model.R2() forecast = model.df_forecast[[&quot;ds&quot;,&quot;yhat_lower&quot;,&quot;yhat_upper&quot;,&quot;yhat&quot;]].tail(30).reset_index().set_index(&quot;ds&quot;).drop(&quot;index&quot;,axis=1) forecast[&quot;yhat&quot;].plot(marker=&quot;.&quot;,figsize=(10,5)) plt.fill_between(x=forecast.index, y1=forecast[&quot;yhat_lower&quot;], y2=forecast[&quot;yhat_upper&quot;],color=&quot;gray&quot;) plt.legend([&quot;forecast&quot;,&quot;Bound&quot;],loc=&quot;upper left&quot;) plt.title(&quot;Forecasting of Next 30 Days Cases&quot;) plt.show() . US Cases Exploration . https://www.kaggle.com/chapagain/covid-19-usa-eda-chart-graph-map/execution . df_us_test = pd.read_csv(&quot;./input/us_covid19_daily.csv&quot;) df_us_states_test = pd.read_csv(&quot;./input/us_states_covid19_daily.csv&quot;) . df_cases_state = pd.read_csv(&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/web-data/data/cases_state.csv&quot;) . state_map_dict = { &#39;AL&#39;: &#39;Alabama&#39;, &#39;AK&#39;: &#39;Alaska&#39;, &#39;AS&#39;: &#39;American Samoa&#39;, &#39;AZ&#39;: &#39;Arizona&#39;, &#39;AR&#39;: &#39;Arkansas&#39;, &#39;CA&#39;: &#39;California&#39;, &#39;CO&#39;: &#39;Colorado&#39;, &#39;CT&#39;: &#39;Connecticut&#39;, &#39;DE&#39;: &#39;Delaware&#39;, &#39;DC&#39;: &#39;District of Columbia&#39;, &#39;D.C.&#39;: &#39;District of Columbia&#39;, &#39;FM&#39;: &#39;Federated States of Micronesia&#39;, &#39;FL&#39;: &#39;Florida&#39;, &#39;GA&#39;: &#39;Georgia&#39;, &#39;GU&#39;: &#39;Guam&#39;, &#39;HI&#39;: &#39;Hawaii&#39;, &#39;ID&#39;: &#39;Idaho&#39;, &#39;IL&#39;: &#39;Illinois&#39;, &#39;IN&#39;: &#39;Indiana&#39;, &#39;IA&#39;: &#39;Iowa&#39;, &#39;KS&#39;: &#39;Kansas&#39;, &#39;KY&#39;: &#39;Kentucky&#39;, &#39;LA&#39;: &#39;Louisiana&#39;, &#39;ME&#39;: &#39;Maine&#39;, &#39;MH&#39;: &#39;Marshall Islands&#39;, &#39;MD&#39;: &#39;Maryland&#39;, &#39;MA&#39;: &#39;Massachusetts&#39;, &#39;MI&#39;: &#39;Michigan&#39;, &#39;MN&#39;: &#39;Minnesota&#39;, &#39;MS&#39;: &#39;Mississippi&#39;, &#39;MO&#39;: &#39;Missouri&#39;, &#39;MT&#39;: &#39;Montana&#39;, &#39;NE&#39;: &#39;Nebraska&#39;, &#39;NV&#39;: &#39;Nevada&#39;, &#39;NH&#39;: &#39;New Hampshire&#39;, &#39;NJ&#39;: &#39;New Jersey&#39;, &#39;NM&#39;: &#39;New Mexico&#39;, &#39;NY&#39;: &#39;New York&#39;, &#39;NC&#39;: &#39;North Carolina&#39;, &#39;ND&#39;: &#39;North Dakota&#39;, &#39;MP&#39;: &#39;Northern Mariana Islands&#39;, &#39;OH&#39;: &#39;Ohio&#39;, &#39;OK&#39;: &#39;Oklahoma&#39;, &#39;OR&#39;: &#39;Oregon&#39;, &#39;PW&#39;: &#39;Palau&#39;, &#39;PA&#39;: &#39;Pennsylvania&#39;, &#39;PR&#39;: &#39;Puerto Rico&#39;, &#39;RI&#39;: &#39;Rhode Island&#39;, &#39;SC&#39;: &#39;South Carolina&#39;, &#39;SD&#39;: &#39;South Dakota&#39;, &#39;TN&#39;: &#39;Tennessee&#39;, &#39;TX&#39;: &#39;Texas&#39;, &#39;UT&#39;: &#39;Utah&#39;, &#39;VT&#39;: &#39;Vermont&#39;, &#39;VI&#39;: &#39;Virgin Islands&#39;, &#39;VA&#39;: &#39;Virginia&#39;, &#39;WA&#39;: &#39;Washington&#39;, &#39;WV&#39;: &#39;West Virginia&#39;, &#39;WI&#39;: &#39;Wisconsin&#39;, &#39;WY&#39;: &#39;Wyoming&#39; } state_code_dict = {v:k for k, v in state_map_dict.items()} state_code_dict[&quot;Chicago&quot;] = &#39;Illinois&#39; def correct_state_names(x): try: return state_map_dict[x.split(&quot;,&quot;)[-1].strip()] except: return x.strip() def get_state_codes(x): try: return state_code_dict[x] except: return &quot;Others&quot; def get_state_name(x): try: for name, code in state_code_dict.items(): if code == x: return name return &#39;Others&#39; except: return &quot;Others&quot; . df_us_states = df_cases_state[df_cases_state[&#39;Country_Region&#39;] == &#39;US&#39;].copy() df_us_states[&quot;State_Code&quot;] = df_us_states[&quot;Province_State&quot;].apply(lambda x: get_state_codes(x)) . import plotly.graph_objects as go . fig = go.Figure(data=[go.Pie(labels=df_us_states[&#39;Province_State&#39;], values=df_us_states[&#39;Confirmed&#39;], hole=.35, textinfo=&#39;label+percent&#39; ) ]) fig.update_layout( title_text=&quot;US Confirmed Cases by States&quot;, # Add annotations in the center of the donut pies. annotations=[ dict(text=&#39;Confirmed&lt;br&gt;Cases&#39;, showarrow=False), ] ) fig.update_traces(textposition=&#39;inside&#39;) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . fig = go.Figure(data=[ go.Pie(labels=df_us_states[&#39;Province_State&#39;], values=df_us_states[&#39;Deaths&#39;], hole=.35, textinfo=&#39;label+percent&#39; ) ]) fig.update_layout( title_text=&quot;US Deaths Cases by States&quot;, # Add annotations in the center of the donut pies. annotations=[ dict(text=&#39;Deaths&lt;br&gt;Cases&#39;, showarrow=False), ] ) fig.update_traces(textposition=&#39;inside&#39;) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . df_confirmed_top = df_us_states.sort_values(&#39;Confirmed&#39;, ascending=False).head(10) fig = go.Figure(data=[ go.Bar(name=&#39;Confirmed&#39;, x=df_confirmed_top[&#39;Province_State&#39;], y=df_confirmed_top[&#39;Confirmed&#39;], text=df_confirmed_top[&#39;Confirmed&#39;], texttemplate=&#39;%{text:.2s}&#39;, textposition=&#39;outside&#39;), go.Bar(name=&#39;Deaths&#39;, x=df_confirmed_top[&#39;Province_State&#39;], y=df_confirmed_top[&#39;Deaths&#39;], text=df_confirmed_top[&#39;Deaths&#39;], texttemplate=&#39;%{text:.2s}&#39;, textposition=&#39;outside&#39;), go.Bar(name=&#39;Recovered&#39;, x=df_confirmed_top[&#39;Province_State&#39;], y=df_confirmed_top[&#39;Recovered&#39;], text=df_confirmed_top[&#39;Recovered&#39;], texttemplate=&#39;%{text:.2s}&#39;, textposition=&#39;outside&#39;), #go.Bar(name=&#39;People Tested&#39;, x=df_confirmed_top[&#39;Province_State&#39;], y=df_confirmed_top[&#39;People_Tested&#39;], # text=df_confirmed_top[&#39;People_Tested&#39;], texttemplate=&#39;%{text:.2s}&#39;, textposition=&#39;outside&#39;), ]) # Change the bar mode fig.update_layout( title_text=&quot;Top 10 US States with Confirmed Cases&quot;, barmode=&#39;group&#39;, #legend_orientation=&quot;h&quot;, yaxis_type=&#39;log&#39;, yaxis_title=&#39;Cases Count in Log Scale&#39; ) fig.update_layout(legend_orientation=&quot;h&quot;, legend=dict(x=0, y=1.1)) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . df_us_states . df_us_states_test[&quot;state_name&quot;] = df_us_states_test[&quot;state&quot;].apply(lambda x: get_state_name(x)) df_us_states_test.head(2) . df_t = df_us_states_test.groupby(&#39;date&#39;).sum().reset_index() df_t . fig = go.Figure() fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;positive&#39;], mode=&#39;lines&#39;, name=&#39;Confirmed&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;death&#39;], mode=&#39;lines&#39;, name=&#39;Deaths&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;recovered&#39;], mode=&#39;lines&#39;, name=&#39;Recovered&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;hospitalized&#39;], mode=&#39;lines&#39;, name=&#39;Hospitalized&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;total&#39;], mode=&#39;lines&#39;, name=&#39;Total Tested&#39;)) fig.update_layout( xaxis_title=&quot;&quot;, yaxis_title=&quot;Cases Count in Log Scale&quot;, title = &#39;Time Series - Confirmed, Deaths &amp; Recovered Cases in USA&#39;, yaxis_type=&#39;log&#39; ) fig.update_layout( legend=dict( x=0, y=1, traceorder=&quot;normal&quot;, #bgcolor=&quot;LightSteelBlue&quot;, bordercolor=&quot;silver&quot;, borderwidth=1 ) ) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . fig = go.Figure() fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;recovered&#39;]/df_t[&#39;positive&#39;]*100, mode=&#39;lines&#39;, name=&#39;Recovery Rate&#39;)) fig.add_trace(go.Scatter(x=df_t[&#39;date&#39;], y=df_t[&#39;death&#39;]/df_t[&#39;positive&#39;]*100, mode=&#39;lines&#39;, name=&#39;Death Rate&#39;)) fig.update_layout( xaxis_title=&quot;&quot;, yaxis_title=&quot;Recovery/Death Rate Percentage (%)&quot;, title = &#39;Time Series - Recovery and Death Rate in USA&#39;, yaxis_type=&#39;log&#39; ) fig.update_layout( legend=dict( x=0, y=1, traceorder=&quot;normal&quot;, #font=dict( # family=&quot;sans-serif&quot;, # size=12, # color=&quot;black&quot; #), #bgcolor=&quot;LightSteelBlue&quot;, bordercolor=&quot;silver&quot;, borderwidth=1 ) ) fig.update_layout(margin={&quot;r&quot;:0,&quot;l&quot;:0,&quot;b&quot;:0}) fig.show() . df_t = df_us_states_test.groupby([&#39;state_name&#39;, &#39;date&#39;]).sum().reset_index() df_t[&quot;date_reported&quot;] = pd.to_datetime(df_t[&quot;date&quot;]).dt.strftime(&#39;%m/%d/%Y&#39;) df_t[&#39;state&#39;] = df_t[&#39;state_name&#39;].apply(lambda x: get_state_codes(x)) # while calculating mortality rate, adding 1 to confirmed to avoid divide by zero df_t[&#39;mortality_rate&#39;] = df_t[&#39;death&#39;] / (df_t[&#39;positive&#39;]+1) * 100 df_t2 = df_t.groupby([&#39;date&#39;, &#39;state_name&#39;]).max().reset_index() df_t2.sort_values(&#39;date&#39;, ascending=False).head() . fig = px.choropleth(df_t2, locations=&quot;state&quot;, locationmode=&#39;USA-states&#39;, scope=&quot;usa&quot;, hover_name=&quot;state_name&quot;, hover_data=[&quot;positive&quot;, &quot;death&quot;, &quot;recovered&quot;], animation_frame=&quot;date&quot;, color=np.log(df_t2[&quot;positive&quot;]+1), title=&quot;US COVID-19 Progression Animation Over Time&quot;, color_continuous_scale=px.colors.sequential.Plasma, ) #fig.update_coloraxes(colorscale=&quot;hot&quot;) #fig.update(layout_coloraxis_showscale=False) fig.update_coloraxes(colorbar_title=&quot;Color&lt;br&gt;Confirmed Cases&lt;br&gt;in Log Scale&quot;) fig.show() .",
            "url": "https://chenghsu.github.io/Data-Analysis-Blog/jupyter/2021/04/11/covid-prediction.html",
            "relUrl": "/jupyter/2021/04/11/covid-prediction.html",
            "date": " • Apr 11, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://chenghsu.github.io/Data-Analysis-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://chenghsu.github.io/Data-Analysis-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://chenghsu.github.io/Data-Analysis-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://chenghsu.github.io/Data-Analysis-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}